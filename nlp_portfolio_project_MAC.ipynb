{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> Text Message Topics in the <br>National University of Singapore SMS Corpus Data Set </h1>\n",
    "<h2 style=\"text-align:center\"> A Codecademy Pro Portfolio Project </h2> <br>\n",
    "This project serves two primary purposes. The first and most immediate purpose is to fulfill the requirements of the Codecademy Pro skill path \"Apply Natural Language Processing with Python.\" The second, and ultimately more important purpose, is to practice my natural language processing skills. As a physicist, the idea of converting words into numbers via a bag of words and/or word embeddings model is fascinating, and then to be able to use those quantitative models for sentiment analysis, categorical prediction, and other applications is quite exciting. The Codecademy skill path covers only the basics of natural language processing, and it is important to have a solid understanding of the basics before moving on to more complicated applications; still, the idea of delving deeper into using more complicated algorithms for natural language processing is intriguing, and I want to get a better sense of how my quantitative skills can be applied to the computer-based processing of human language. <br><br>\n",
    "The data set used in this project was provided by Codecademy; it is the National University of Singapore SMS Corpus data set, made publicly available by The National University of Singapore. It was compiled by Tao Chen and Min-Yen Kan, as detailed in the following publication: <br>Tao Chen and Min-Yen Kan (2013). Creating a Live, Public Short Message Service Corpus: The NUS SMS Corpus. Language Resources and Evaluation, 47(2)(2013), pages 299-355. <a href = https://link.springer.com/article/10.1007%2Fs10579-012-9197-9> Paper URL </a> <br>I chose to use the National University of Singapore SMS Corpus data set rather than one of the other options that Codecademy provided because it posed two main challenges. The first and most intriguing challenge to me was the informal nature of the corpus: word abbreviations, sometimes to the point of single letters, as well as the use of numbers and/or symbols in place of letters are ubiquitous in text messages, and I wanted the challenge of working with \"messy\" data to see if I could determine common text message topics in the face of non-standard language usage. The second challenge was dealing with a mix of English and non-English words. I know that English is a very commonly spoken language in Singapore, and I am a native English speaker myself, but I know that many Singaporeans also know Malay, Tamil, and/or Mandarin Chinese; it is reasonable to expect that when sending text messages, a mix of these languages—perhaps even within a single sentence—might be used, and I wanted to see if I could still make broad sense of the corpus with these non-English words present. The above two challenges are no doubt common in the world of natural language processing, so I might as well learn to be comfortable navigating them as early as possible. <br><br>\n",
    "My central questions regarding the National University of Singapore SMS Corpus data set are the following:<br>\n",
    "<ol>\n",
    "    <li> Text message senders from which countries send the longest messages? </li>\n",
    "    <li> What are some of the most common topics of text messages sent by people from Singapore, India, and the United States? How are they similar, and how are they different?</li>\n",
    "    <li> How (if at all) did the common topics of text messages sent by people from Singapore change between 2003 and 2011?</li>\n",
    "    <li> Is it possible to use text message data to predict what country a sender is from?</li>\n",
    "</ol>\n",
    "The rest of this notebook aims to answer these three questions using such tools as word counts, TF-IDF analysis, word embeddings, n-gram identification, and a Multinomial Naive Bayes supervised machine learning model. It is broken down into the following sections:<br>\n",
    "<ul>\n",
    "    <li> <a href='#Section1'> Importing Needed Software Libraries</a> </li>\n",
    "    <li> <a href='#Section2'> Reading in the Data and Cleaning It </a> </li>\n",
    "    <li> <a href='#Section3'> Exploratory Data Analysis </a> </li>\n",
    "    <li> <a href='#Section4'> Text Pre-Processing </a> </li>\n",
    "    <li> <a href='#Section5'> Average Text Lengths Per Country </a> </li>\n",
    "    <li> <a href='#Section6'> Important Words in Messages: TF and TF-IDF Analysis </a> </li>\n",
    "    <li> <a href='#Section7'> Important Words in Messages: TF and TF-IDF Analysis Comparing and Contrasting Texts from Singapore Eight Years Apart </a> </li>\n",
    "    <li> <a href='#Section8'> Word Embedding Analysis Based on TF-IDF Analysis </a> </li>\n",
    "    <li> <a href='#Section9'> The Most Common Two-, Three-, and Four-Word Groups of Words </a> </li>\n",
    "    <li> <a href='#Section10'> Using Message Text to Predict Country of Origin </a> </li>\n",
    "    <li> <a href='#Section11'> Conclusions </a> </li>\n",
    "    <li> <a href='#Section12'> What I Would Do Differently </a> </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Importing Needed Software Libraries </h3> <br>\n",
    "Several software libraries will be used to clean the data, normalize it, visualize it, and conduct TF-IDF analysis, word embedding analysis, and n-gram identification. The scikit-learn library will be used to form a Multinomial Naive Bayes supervised machine learning model of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas (to organize and clean the data):\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries (to visualize the data):\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mcale\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Import regex operations library (to normalize the data):\n",
    "import re\n",
    "\n",
    "# Import machine learning library for word embedding (to analyze the data):\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Import Counter class of collection containers library (to analyze the data):\n",
    "from collections import Counter\n",
    "\n",
    "# Import classes and methods for text tokenizing, lemmatization, stopwords removal, and n-gram evaluation:\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import classes and methods for bag-of-words model building, TF-IDF text analysis, and Multinomial Naive Bayes \n",
    "# model building:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Import model performance-measuring methods:\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I installed Levenhstein as indicated in the warning message above, but yet the warning message still appears. I do not know why this is.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Reading in the Data and Cleaning It </h3> <br>\n",
    "The data set must be read into Python before any analysis can be done. I will read it into a Pandas DataFrame, because this data organization structure is easy to work with and very flexible in terms of the data structures it permits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48598 entries, 0 to 48597\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  48598 non-null  int64 \n",
      " 1   id          48598 non-null  int64 \n",
      " 2   Message     48595 non-null  object\n",
      " 3   length      48598 non-null  object\n",
      " 4   country     48598 non-null  object\n",
      " 5   Date        48598 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 2.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read the .csv data file into a Pandas DataFrame:\n",
    "sms_text_data = pd.read_csv('clean_nus_sms.csv')\n",
    "\n",
    "# Print out basic DataFrame information:\n",
    "print(sms_text_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codecademy provided the following schema for the data set:\n",
    "<table>\n",
    "   <th>Feature</th>\n",
    "   <th style=\"text-align:left\"> \n",
    "       Description</th>\n",
    "   <tr>\n",
    "     <td>id</td>\n",
    "     <td style=\"text-align:left\"> Unique identifier for each message.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Message</td>\n",
    "      <td style=\"text-align:left\"> The message contents.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>length</td>\n",
    "      <td style=\"text-align:left\"> Total number of characters in the message.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "     <td>country</td>\n",
    "     <td style=\"text-align:left\"> Country the sender is from.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "     <td>Date</td>\n",
    "     <td style=\"text-align:left\"> Month and Year a message was sent, in YYYY/M format.\n",
    "</td>\n",
    "   </tr>\n",
    "</table>\n",
    "<br>Without even looking at any of the text messages stored in the Message column, it is clear that some initial data cleaning must be done, in particular to eliminate null values and to remove the unneeded column \"Unnamed: 0.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the records have a null value in the Message column. Since my goal is to investigate the contents of the\n",
    "# text messages themselves, these records are useless. Drop them from the DataFrame:\n",
    "sms_text_data = sms_text_data.dropna(subset=['Message'])\n",
    "\n",
    "# The first column appears to be a running index designed to match the index of a row in a Pandas DataFrame. \n",
    "# This column is redundant, so I drop it from the DataFrame:\n",
    "sms_text_data.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional cleaning must be done. An initial investigation of the data set reveals that some \"text messages\" are actually error codes; further, lengths are listed as objects rather than integers, and the Date column can be split into separate year and month columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 48591 entries, 0 to 48597\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       48591 non-null  int64 \n",
      " 1   Message  48591 non-null  object\n",
      " 2   length   48591 non-null  int64 \n",
      " 3   country  48591 non-null  object\n",
      " 4   Date     48591 non-null  object\n",
      " 5   year     48591 non-null  object\n",
      " 6   month    48591 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 3.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Four text messages have an error message instead of a length. Conveniently, the Message text of these messages\n",
    "# is the identical error message. Remove these four messages from the DataFrame:\n",
    "\n",
    "sms_text_data = sms_text_data[sms_text_data.length != 'Err:511']\n",
    "sms_text_data = sms_text_data[sms_text_data.length != 'Err:510']\n",
    "sms_text_data = sms_text_data[sms_text_data.length != 'Err:509']\n",
    "sms_text_data = sms_text_data[sms_text_data.length != 'Err:508']\n",
    "\n",
    "# Convert the length column from type object to type integer:\n",
    "sms_text_data.length = pd.to_numeric(sms_text_data.length)\n",
    "\n",
    "# Split the Date column into a year column and a month column. Add these to the existing DataFrame.\n",
    "# Since I will not be doing any calculations with either of these columns, I will keep them as type\n",
    "# object rather than convert them to type integer.\n",
    "sms_text_data['year'] = sms_text_data.Date.str[0:4]\n",
    "sms_text_data['month'] = sms_text_data.Date.str[5:]\n",
    "\n",
    "# Print out DataFrame information again, to make sure nothing blatantly obvious has gone wrong:\n",
    "print(sms_text_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section3\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Exploratory Data Analysis </h3> <br>\n",
    "While the data set schema provided by Codecademy gave me a good idea of what each feature represents, there are several things about the data that I would like to know before beginning the project in earnest. These questions are:\n",
    "<ol>\n",
    "    <li> What countries are the text message senders from? </li>\n",
    "    <li> How many messages were sent by senders from each country? </li>\n",
    "    <li> How many text messages, regardless of sender country, were sent each year? </li>\n",
    "</ol>\n",
    "These questions can be answered with some basic exploratory data analysis, so I conduct such analysis here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> What countries are the text message senders from? </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SG' 'MY' 'India' 'Pakistan' 'United States' 'Canada' 'unknown' 'Serbia'\n",
      " 'United Kingdom' 'France' 'Singapore' 'China' 'Malaysia' 'Sri Lanka'\n",
      " 'Spain' 'Bangladesh' 'Philippines' 'Macedonia' 'Romania' 'Kenya'\n",
      " 'Slovenia' 'New Zealand' 'Nigeria' 'Ghana' 'Indonesia' 'Nepal' 'Morocco'\n",
      " 'USA' 'Lebanon' 'india' 'Trinidad and Tobago' 'INDIA' 'srilanka'\n",
      " 'jamaica' 'Hungary' 'Australia' 'Italia' 'BARBADOS' 'Turkey' 'UK']\n"
     ]
    }
   ],
   "source": [
    "# What countries are represented in the data?\n",
    "print(sms_text_data.country.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the country listings are clearly redundant (e.g., \"USA\" and \"United States\"). So, I will rename those country listings so as to ensure that all Messages sent from a particular country get associated with that country. My convention will be to keep the \"human readable\" version of a country name and convert two-letter country codes and country names in all capital letters or without the first letter capitalized to standard English spelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Singapore' 'Malaysia' 'India' 'Pakistan' 'United States' 'Canada'\n",
      " 'unknown' 'Serbia' 'United Kingdom' 'France' 'China' 'Sri Lanka' 'Spain'\n",
      " 'Bangladesh' 'Philippines' 'Macedonia' 'Romania' 'Kenya' 'Slovenia'\n",
      " 'New Zealand' 'Nigeria' 'Ghana' 'Indonesia' 'Nepal' 'Morocco' 'Lebanon'\n",
      " 'Trinidad and Tobago' 'Jamaica' 'Hungary' 'Australia' 'Italy' 'Barbados'\n",
      " 'Turkey']\n"
     ]
    }
   ],
   "source": [
    "sms_text_data.replace('SG','Singapore', regex = False, inplace = True)\n",
    "sms_text_data.replace('MY','Malaysia', regex = False, inplace = True)\n",
    "sms_text_data.replace(\"srilanka\", \"Sri Lanka\", regex = False, inplace = True)\n",
    "sms_text_data.replace(\"UK\", \"United Kingdom\", regex = False, inplace = True)\n",
    "sms_text_data.replace(\"USA\", \"United States\", regex = False, inplace = True)\n",
    "sms_text_data.replace(\"INDIA\", \"India\", regex = False, inplace = True)\n",
    "sms_text_data.replace(\"india\", \"India\", regex = False, inplace = True)\n",
    "sms_text_data.replace(\"BARBADOS\", \"Barbados\", regex = False, inplace = True)\n",
    "sms_text_data.replace(\"Italia\", \"Italy\", regex = False, inplace = True)\n",
    "sms_text_data.replace(\"jamaica\", \"Jamaica\", regex = False, inplace = True)\n",
    "\n",
    "# Print out the unique values in the country column again, to make sure I didn't miss any unconventional spellings \n",
    "# and/or make some other blatantly obvious error:\n",
    "print(sms_text_data.country.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> How many messages were sent by senders from each country? </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a bar plot of Messages sent per country using Seaborn's countplot() method. I choose to plot the bars in the \n",
    "# order in which they appear in the data rather than order them by decreasing counts.\n",
    "\n",
    "# Set the figure size:\n",
    "plt.figure(figsize=(18,8))\n",
    "# Set the context for the notebook (for font scale primarily):\n",
    "sns.set_context(\"notebook\", font_scale=0.85, rc={\"lines.linewidth\": 1.5})\n",
    "# Generate the bar plot, and make each bar the same colour:\n",
    "sns.countplot(data = sms_text_data, x=\"country\", color = 'red')\n",
    "# Make the x-axis tick marks easier to read, and make sure the axis labels are informative and readable:\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel(\"Country\", fontsize=14)\n",
    "plt.ylabel(\"Number per Country\", fontsize=14)\n",
    "plt.title(\"Number of Text Messages Sent Per Country\", fontsize=18)\n",
    "# Display the plot:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the vast majority of messages were sent by people from Singapore. This is unsurprising, given that the data set is mostly comprised of messages from Singaporeans and students attending the National University of Singapore. (Source: Codecademy.) Over 5,000 messages were sent by people from India; over 5,000 messages were sent by people from the United States; and the remaining 30 countries combined to account for the remainder of the text messages. If I wish to investigate common text message topic differences across countries, I should pick countries with the highest number of messages sent, as they will form the largest corpuses to conduct natural language processing analysis. These countries are Singapore, India, and the United States. I will therefore concentrate most of my analysis on these countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> How many text messages, regardless of sender country, were sent each year? </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2003' '2010' '2011' '2012' '2013' '2014' '2015']\n"
     ]
    }
   ],
   "source": [
    "# What years were the text messages sent?\n",
    "print(sms_text_data.year.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAGZCAYAAAANRdxJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7RkVXnv/e+PBhRFaJAIGoyNt4O83oLEHNK0gCLmGFT0GE8SomJQk+PxRZIQrzjAgIngJRhNfI0viqB4iYL3KBoEW8ELIcYoioKgAVEauxu5iWA/54+5thRF7d61r7VX8/2MscaumjVr1bNm1d77qTnXnCtVhSRJkvpnq0kHIEmSpLkxkZMkSeopEzlJkqSeMpGTJEnqKRM5SZKknjKRkyRJ6ikTOWmWklSSUycdx1wkuUeSv0/ywyS/THLFpGOSJM2diZyWhSQHdAlSJXn+NHUqySeWOrYtzMuA/xf4AHA4cNSoSknOHXg/ZtoOX8gAkxya5LhZPmcq3luT7DZNnTcPxHzAQsQqSPKUJJ9NcmWSW5JcneT8JCcl2WWJYjhqtp/DEZ/jnyf5XpI3Jdl5kUIdfP0VSb6S5MYkD56mzmFdbG9Y7HjUX3FBYC0H3T/Wz3d3fwQ8uKpuHqpTwCer6pAlDu8OujjeXVWHTzKOuUhyPrB9VT1yhnpPBHYdKNoF+DtgLfBPQ9XPr6rvL2CMpwLPrarM4jnnAqu7u6+sqtcPPb4t7XN1T+DuwIFVde5CxHtXluRE4KXAN2hfDn4C3A/4TeDJwH5VdeESxHEFcEVVHTCL5xTwdeCNXdHOtJifBHwTeExV/WJhI71TDP8N+PduW1NVmwYe2w34Fq1N966qny9mLOqvrScdgDTkQmAfWk/R3044lolLsgK4W1XdtEC73A344UyVquqzQ3GsoiVy36+q9yxQLAvtFuAc4HnA64ceexpwb+AM4I+WOK4tUpL7AEcDXwNWV9WtQ4/vCPxyErHNwlVDn+e/T/Jx4BDaZ+af5/sCSe5VVdePeqyqLknyStrv1l9yx8/tPwE7AE9ayiQuyXbArVV121K9pubHoVUtNx8E/g14WZJ7z1R5uvPVkhw+PISW5LiubK8kJ3dDQDcm+dfumzFJnpHkoiQ3J7kiyQs389oHJflykpuS/LgburvniHo7JjkxyaXd0NO6JO9L8sBpYj4oyauTXAb8HHjWDG2wdZKXJbm4Gx76aZKzkjxieN/AHsD+A8NJx21u3+Po4j07ycbu9b+R5M+G6nwg7Zy8A4bKn5RkU5LTuvvnAs/tbs9l+PZdwMOS/PZQ+fOA/6D1fIw6hrsleWWSb3XHsDHJx5P85lC9dMN430hyfZKfJbkkySlJthmo9ztJ/qX7XPw8yVVJPpXkvw/UuV+SNyb5epINXb2Lu/dyxYgYVyX5cPea1yX5aJI9us/puSPqz/i+jBvrNB5I+x/yheEkDqCqrquqG+bYzlOnWhye5Hld/VuS/CDJS4fqFvAA7vi5rrQvH3Pxme7nr4Y7k+zT/U5d28VxSZJXJblDZ0jaEP8VSR6Y5ENJ1gM/m+H13gycBxyfZK9uP88FngL8zVSP5ixieGySU5N8N+1v0/VJvpTk6cMv3NWrJL+W5J1JfgLcCOw+qxbTRNkjp+WmaOdxfQ54FfAXi/Aa7wZuAP4G+DXaN+HPJHk1cBLwNuCdwBHA25NcXFVfHNrH3sAzgXcApwEHAkcCD0/yxKkhkrReifOB3+j2+S3gvsCLgK8k2aeqfjC07zcA23T7/hlwyQzH815asvfZLvbdgP8DXJBkTVX9O/AF4Nm0b/7XAq/tnvuNGfa9WWmJ7v8HfLnb543AE4G3JXlQVf1VV/WFwG8B70ny6Kq6Nm3o6DTgUlp70O1jK2BNF++U88cM6RPANcCfAF/pYrwfcDDts7TtiGPYBvg08DvA6cBbgR2BFwBfSvK4geHBY4C/Bj7eHfcvacnxU4G7AbemfSn4LPBj2j/pn9Dek9XAo7q2Angk8AzgLOAy2nv+P4DX0ZKkPx2I8d60Ye1du9f9dtdGn6cNFw8f01jvyyxiHWVqOP2QJG+qqh9tpu5s23nKn3XHfAqwEfhj4MQkV1bVGV2dUZ9rgHWbi2czHtL9vLaL+8m09+hS2jDsemBf2ufg0cDvDz1/e1pi9iXa37D7bO7Fqmrqi8p/Aqcl+Z/AybQvHSfMIYanA3vSvhT/gNYT/VzgzCSHDbTboKnPwPG0z9MNI+pouaoqN7eJb8ABtCTu6O7+2bTeqAcM1CngE0PPK+DUEfs7vHvsgIGy47qyj9OdH9qVH9mVXw/8xkD5r3UxvG/EaxZw6FD5m7vyPxgquxl41FDdB9CStFNHxHwJcI8x2+2J3XM+MHRMjwRuA9YO1b8COHcO78+q4bamJaQ/B84YUf/NtCTnQQNlvw38omv/rWj/PG6hnYs0+NxT25+mWcV3LnBDd/uNwHXAdt39V3avc2/aUODw5+LPu7InDe1zB9ow9LkDZRcBF88Qy9Tn6bEz1Ntu8D0bKD+9a7v7DpSd1O3zsKG6U+WDMY79vowb62aO4S3d82+hfVk4ifYFZ6cRdWfTzgd0dX8ErBwovwctQbtgvp/rbv+foZ3/uQstgfvz7jO6kZaA3Z2W4HwB2Hqa4xn8LJ3blZ0wh7Z8Qffcdd379/CufLYx3HPEvu9B+7ty8VD5qd3z3zOX999teWwOrWq5ehmt9+T4Rdj331f3V6yztvv50ar61fljVbWO9sfvIdzZJVX1kaGy13U/nw5tGA44jPYH+Koku0xttB6SL9N6ioa9rcY/J25quOS1g8dUVd+g9U7tl+TXxtzXbD2T1gt1yuCxdcc3law9YSCmr9B6tA6htclBwMur6t8WOK530pKDZ3T3D6e9tz+dpv4fA98B/m3oGLalJZv7pZ03BC1B/PUk+23m9a/rfj4tyd2nq1RVN0+9Z0m2TbJz97qfobXdPgPVnwJcDbxvaDejZjPO5n0ZK9bNOBJ4Dq3H9LHAX9HOK7s67XSCwSHi2bTzlHdV1capO93vxZcZ/Ts5FwfTEqd1wHeBNwEXAwdX1TW0L0q70obsVw7F/amBfQyb9SzTqnoH7QvsLsBrquqb3UOziqGqbpy6nbbc0L1pidw5tNMOdliIeLV8OLSqZamq/j3J+4DDkryhS0wWyvAMyw3dz8tH1N1A6z0b9u3hgqq6OslG2rAYtB69e3P7P4tRNo0o++40dUfZo9vHneKhzbx7WldnrsNMm/Ow7ufnNlNn16H7r6clcmto/7ROXuigqupbSb4GPC/JD2n/9F+ymac8jNY7trk22gX4L1rv3keAtUl+ROuB+STwobp9huP7aUnLK4E/T/JlWnL2/hoYRu/ObXo5LRF6MDA8S3engdt7AF+tgVmN3bFe033mho8Hxntfxop1Ol0iejpwetrM4EfSPu9H0WazbuT2SUuzaecpo2ZD/5T2e7UQpr5cQOtV/MHglzlub8t3bmYfw5/xdYPJ5yxdQGu/C+YaQ9oklBNov/ujhnVXcufz9mbzN0fLjImclrNjaL0LJ9LOHZqNzX22p5tJN135qGUwplu3JyNuf452DOOazQzVsZfoWARTr/0cWm/RKMP/iFfR/tlDS162pw1pL7R3Av/Y3b6KljROJ7TzkzZ3PuY6gKq6IMmDaEtUHNhtfwQck2S/qlpfVbcAT0zy2K7e42jnMh2X5I+q6qxun2/i9jX9Xks7t+9W2vmXJzL3yWhjvy+ziHVGXSJ7IXBhkg/Tvlwcwe2J3NjtPGCxZ71eW1WbS3in2vKvaEuVjDJ8buBCzTCfdQzdKMDZtOTv72kziq+jtePzaJ/VO32uZjECoGXIRE7LVlVdnuRtwEuSHDhNtfW09Z+GPXBE2ULaa7ggyX1pJ29PJS/raD0SO8zwz2I+LqP9A34Yd564MBXjqJ7GhfC97udM/wyBX/VAvY/2d+dI2vlab6P1CA1aiMUt30dLlJ5Am/m3uYTge7Te03OGe7xGqTYT88PdRpIXAf9AS1peP1Dvq8BXuzr35/aT16eSo2fTZnz+weD+M3px2CuAByfZqu641th9aD0sw8cDY74vY8Y6K9WW1dgA/PpQXGO382xfcoH3N2WqLW9cxN/hhYzhkbRJKn9dVccOPpBpFlpX/3mOnJa7E2jDANP1aH0X2DfJPaYKkuxE+/a5mP5bkkOHyl7W/fwIQPfP6r3AY5M8c9ROun/E8zF1nt4rum/jU/t9OG0m5Re7c/0Wwwdpw1GvGXFu09SyK3cbKDqBNuHhxVX1FtqkhMPSlloYdEP3/Dmvrl9V19FmPL4GePsM1U+jzdQc2VOUZHDYatSVCi7qfu68mTpX0hL7wWP6JUM9qmnL1/z5iOd/nDaJ4Q+Hyo8eUXfs92UWsd5Jkt2SPHqax9Z0z794oHjsdp6DG5gh3jn6DK2n9OWjPo9Jtktyr0V43bnGMPWFZfhz9XBuP59WWxh75LSsVVum4vVMP+nhrcB7gHOSnE7rnXgBbdr9yEs1LZD/pC2l8Q7aN+YDacPA59GGyqa8iraUwweTfJB2ovYvaOfdPZm2Zt7hcw2iqj7b7fcPgJ3SLmE2tfzIz2k9X4uiqq5M8r+B/x/4dtf+P6D1ujwCOJTWK3hFkoNo50ydUVWndrt4JbA/8NYk51fVVM/Dl4EXA/+Y5JO04cavVNWsehar6rQxq76ZdkL565M8nnZS+M9oS8Y8gdaOUz3C3+7OI/sKbTjrvrSlVX5BO98M2jDrwbTJJpfT/qk+hbYkxEkDr/sh4E+TfIA2/L4rbdmUUZMyTqQNi72rGwb9DrAf7bN1LQM9UrN5X2YR6yi7A19L8hXgX2k90Xej9QgdRnvfXjlQfzbtPFtfBo5IcjxtSHcT8PHBE//noqpuTPIc2hemS5K8k7YEyEpaGz2DliCdO5/XWcAYvk1b4uil3ZfbS4CH0pay+SZt2F5bmklPm3Vzq7rz8iNDj92D9k/zTsuPdI//Fe0f1S20P2R/wuaXH1k19PxVXflxI/Z9Lu3SP4NlRZu2fxDtH/rNtPW33gLca5r4X01L/m6mnRP2bdo6cb89UO9OMY/ZdlvTegO/3bXBetof/UeMqHsFC7T8yMBjq2lDcNfQEpof0dY3+0va0gn3oZ2rdelw+wAPov0zvxDYtivbijaL7kpaD0MBh88Q37l0y4/MUO9Oy48MtOGRtHOKbuy279F6VA8eqPdy2ozba7q2/i/aLM29hz7LH+ja+ubu/fgK8HzuuETMPWhDsT+gJTHf6/b/hFHHTJvwcGb3+fkZ8NGu7FrgU7N9X2YT6zRtuT1t/b+pdfBu6NrkCtqXq9+c5rM6TjsfMN37zojlabrP2Ie7+Dcx4vd8xH5G/j2Zpu7Du2O6qmvLn9Bm6r4a2Hlzfy9m+Xt23KjP5yxjeED3mVxHO1/vq7REb2rfqwbq3qkt3fq3ea1VSeqpbmmJa4G3V9WdrtogacvnOXKS1AOjznfj9vMyPzviMUl3AfbISVIPpF1P9Qe0YegVtCHYQ2jDa4+rzc/MlbSFMpGTpB5I8pe0teFW0RbWvZJ2ztxrqmox1uKT1AMmcpIkST11l1t+pFs/6bdos+gcipAkScvZCtpSR1+rdjWWO1iyRC7JQ2lTnW+lrXb/h7TLjUxd3uTYqjovyZ60ZRm2Bl5aVWuTbE+7nt99gH+uqpO7fZ4E7Etb1uD5Y54j8lvcfpF0SZKkPlgDfHG4cCl75L4PrK6qSnIsbdX59VV1wFC919LW05paC2t/2qVvPlZV70pyTpL30C79smtVrUlyAu2k34+OEcfVAGvXrmX33XdfgMOSJElaHFdeeSVr1qyBaa6dvGSJXFXdNnB3G9qK0zsmOQ/4Ie2yPdcBu1XVZQBJbu2m3K/m9svWnAPsQ7uW5tSFsD/NiEQuyUrufB3C+wLsvvvurFq1amEOTpIkaXGNHHVc0nXkkjw+yUW0y7BcDuxXVfvTVsN+xVS1gadsBHaiJWPXDZTtPE3ZsKO61xncHFaVJElbhCVN5KrqnKramzZk+vyqmrqm4JnAI7vbmwaesiOwgZao7TBG2bCTaZewGdzWLMjBSJIkTdiSJXLdbNEpPwNuHSjbjzZhAeCaJKu6YdFtq+pm2oKXB3WPH0hbEHOw7GDgguHXrKqNVXXF4EZbe0mSJKn3lnKyw/5JXkXrcVsP/AVwQZIbaBf2PbyrdwxthuoKbh9uPQU4PckLgTOrah2wLsn6JGtpEymOX7IjkSRJWgaWcrLD2dw+OWHK3iPqXczQ8Ge3avmhI+oevZAxSpIk9cmSniMnSZKkhWMiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FNLeWUHSTPYY48rJh3CRF1++apJhyBJvWKPnCRJUk+ZyEmSJPWUiZwkSVJPmchJkiT1lImcJElST5nISZIk9ZSJnCRJUk+ZyEmSJPWUiZwkSVJPmchJkiT1lImcJElST5nISZIk9ZSJnCRJUk+ZyEmSJPWUiZwkSVJPmchJkiT1lImcJElST5nISZIk9ZSJnCRJUk+ZyEmSJPWUiZwkSVJPmchJkiT1lImcJElST5nISZIk9ZSJnCRJUk+ZyEmSJPWUiZwkSVJPmchJkiT1lImcJElSTy1ZIpfkoUnOT3Jeko8muUeSZ3Vln0uyW1dvzyRrk1yQZE1Xtn2Ss5J8KclRA/s8qav7riQrlupYJEmSloOl7JH7PrC6qvYHLgKeARwFHAC8DnhZV++1wOHAk4ETurIjgI9V1WrgqUl2SfIoYNeqWgNcBRyyRMchSZK0LCxZIldVt1VVdXe3AS4HvlVVvwDOAR7TPbZbVV1WVRuAW5NsB6wGzu4ePwfYZ6js08C+w6+ZZGWSVYMbsPvCH50kSdLSW9Jz5JI8PslFwIFAAdcBVNUmYOupagNP2QjsBKycqtuV7TxN2bCjaAnj4LZ2gQ5HkiRpopY0kauqc6pqb+AjwEHADgBJtgJu66ptGnjKjsAGWqK2wxhlw04G9hja1izQ4UiSJE3UUk52uNvA3Z/RetP2SrINMHXeHMA13TDoSmDbqroZOJ+W+EHrzbtwqOxg4ILh16yqjVV1xeAGXLnAhyZJkjQRW89cZcHsn+RVtB639cBzgJ8A5wE3Ac/u6h0DnA6sAF7RlZ0CnJ7khcCZVbUOWJdkfZK1tIkUxy/ZkUiSJC0DS5bIVdXZ3D45Ycr7u22w3sUMDX9W1fXAoSP2efQChylJktQbLggsSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPbVkiVySRyY5P8l5Sd6XZOsk301ybrft39XbM8naJBckWdOVbZ/krCRfSnLUwD5P6uq+K8mKpToWSZKk5WApe+R+DBxcVfsDlwNPAdZX1QHddl5X77XA4cCTgRO6siOAj1XVauCpSXZJ8ihg16paA1wFHLKExyJJkjRxS5bIVdU1VXVDd/dW4JfAjl0P3elJduwe262qLquqDcCtSbYDVgNnd4+fA+wzVPZpYN8lORBJkqRlYsnPkUtyf+Bg4F+A/boeunOBV0xVGai+EdgJWAlcN1C28zRlw6+1MsmqwQ3YfSGPR5IkaVKWNJFLck/gNOB5VXVrVf20e+hM4JHd7U0DT9kR2EBL1HYYo2zYUbRh3MFt7YIcjCRJ0oQt5WSHrYD3ACdW1XeSbJvkbt3D+wGXdrev6XrPVgLbVtXNwPnAQd3jBwIXDpUdDFww4mVPBvYY2tYs7JFJkiRNxtZL+FpPAw4AdkrycuBtwMuS3ADcRJvgAHAMcDqwgtuHW08BTk/yQuDMqloHrEuyPsla4PvA8cMvWFUbaT13v5JkuJokSVIvLVkiV1VnAWcNFX9gRL2LGeo1q6rrgUNH1D16IWOUJEnqExcEliRJ6ikTOUmSpJ4ykZMkSeopEzlJkqSemjGR666J+uQk916KgCRJkjSeGRO5qrqNtmDvvRY/HEmSJI1r3KHV/wAevJiBSJIkaXbGTeSOA96Y5NAk90+y8+C2iPFJkiRpGuMuCPzJ7ueZQA2Up7u/YiGDkiRJ0szGTeQOXNQoJEmSNGtjJXJVdd5iByJJkqTZmdW1VpPcD/gNYNvB8qr6wkIGJUmSpJmNlch1CdwZwONo58RNnRs3xXPkJEmSlti4s1ZPBn4J7AXcBKwBfh/4NvC7ixOaJEmSNmfcodX9gd+rqu8kKWBdVX0pyS3A8cBnFy1CSZIkjTRuj9x2wLXd7fXAfbrbFwOPXOigJEmSNLNxE7nvAHt2t78O/FmSBwD/B7hqMQKTJEnS5o07tPpmYLfu9l8Dnwb+ELgFeO4ixCVJkqQZjLuO3HsHbl+UZBWth+6HVXXtdM+TJEnS4pnVOnIASXalTXa4aBHikSRJ0pjGOkcuyTZJTkpyPe2cuFVd+YlJXrSI8UmSJGka4052OBZ4CvDHtPPipnwVOHyBY5IkSdIYxh1a/UPgT6rqvCSbBsq/CTx04cOSJEnSTMbtkbsf8IMR5Vszh/PsJEmSNH/jJnLfol1nddizgH9buHAkSZI0rs32piXZuqpuA14DvCfJ/YEVwO8n2RP4I+D3Fj9MSZIkDZupR+5rSfaqqo/Tet8OBjbRJj88BHhKVX1ukWOUJEnSCDOd3/ZN4MIkr66qNwKfWYKYJEmSNIbN9shV1bNpw6dHJzm3u76qJEmSloEZZ5xW1UeSfBH4B+AbSd4L3DZU58hFik+SJEnTGHfpkA3At4FnAI/gjolcLXRQkiRJmtmMiVySvYDTgF2AJ1XVOYselSRJkma02XPkkhxNWyfuYuCRJnGSJEnLx0w9ci8FDquqM5ciGEmSJI1vpkTuEVX1kyWJRJIkSbMy0/IjJnGSJEnL1LjXWpUkSdIyYyInSZLUUzMmckm2SrJXknvO54WSPDLJ+UnOS/K+JFsneVZX9rkku3X19kyyNskFSdZ0ZdsnOSvJl5IcNbDPk7q670qyYj7xSZIk9c04PXIFfB247zxf68fAwVW1P3A58HTgKOAA4HXAy7p6rwUOB54MnNCVHQF8rKpWA09NskuSRwG7VtUa4CrgkHnGJ0mS1CszJnJVVcAlwK/N54Wq6pqquqG7eystMfxWVf0COAd4TPfYblV1WVVtAG5Nsh2wGji7e/wcYJ+hsk8D+84nPkmSpL4Z9xy5lwKvT/LoJJnPCya5P3Aw8B/AdQBVtYnbl0IZ3P9GYCdg5VTdrmznacqGX2tlklWDG7D7fOKXJElaLsa91uoHgbvTrvJwW5JbBh+sqh3G2Ul3nt1pwPNoSeQOXflW3H791k0DT9mRdp3XjV3dG7qyy2iJ3A5D9YYdBRw7TmySJEl9M24i9+L5vlCXrL0HOLGqvpNkG2Cv7ud+wEVd1Wu6nrONwLZVdXOS84GDaEnggcA/AVcDRwJn0Hr4LhjxsicDpw6V7Q6sne/xSJIkTdpYiVxVvXsBXutptIkNOyV5OfDWbjsPuAl4dlfvGOB0YAXwiq7sFOD0JC8EzqyqdcC6JOuTrAW+Dxw/Iu6NtITwV+Y5MixJkrRsjNsjR5JdacnWg4BXV9W1SVYDP6qqy2d6flWdBZw14qH3D9W7GFgzVHY9cOiIfR49bvySJElbmrEmOyR5DG3m6mG0pUCmzk17Im25EEmSJC2xcWetvgF4c1X9JjA40eEztGVAJEmStMTGTeQeA4w6T+5qYNeFC0eSJEnjGjeRu5m2ntuwPYFrFi4cSZIkjWvcRO6jwLFJ7tbdr26JkBOBDy9CXJIkSZrBuInc0bQrJ6wD7gF8EbiUtrTHMYsTmiRJkjZn3HXkfgbsl+TxwN60BPCiqvrcYgYnSZKk6Y29jhxAVZ1Du2i9JEmSJmzcoVWSHJrkC0mu7ba1SZ6+mMFJkiRpeuMuCPyXwAdoiwK/tNu+A5yRxKsrSJIkTcC4Q6tHAy+uqncMlL0zyVeBv6YtGCxJkqQlNO7Q6vbA50eUf757TJIkSUts3ETuI8AzR5T/T+BjCxeOJEmSxjXu0OqlwMuTHAhc0JX99257U5K/mKpYVW9a2BAlSZI0yriJ3OHABuCh3TZlA/C8gfsFmMhJkiQtgXEXBN5jsQORJEnS7Iy9jpwkSZKWFxM5SZKknjKRkyRJ6ikTOUmSpJ6aMZFLsnWSFyW531IEJEmSpPHMmMhV1W3A64FtFj8cSZIkjWvcodUvA3svZiCSJEmanXEXBH4H8MYkDwD+Dbhx8MGqumihA5MkSdLmjZvIndH9HHXVhgJWLEw4kiRJGte4iZxXdpAkSVpmxr1E1w8WOxBJkiTNztjryCX5H0k+keTiJPfvyp6f5AmLF54kSZKmM1Yil+Qw4IPA92jDrFNLkawAXro4oUmSJGlzxu2Reynwgqr6c+C2gfIvA49e8KgkSZI0o3ETuYcAF4wovwHYYeHCkSRJ0rjGTeR+BDx0RPnjgMsWLhxJkiSNa9xE7p+Av0+yurt//yTPBU4C3rYokUmSJGmzxl1+5KQkOwKfBe4OfB64BXhDVf3DIsYnSZKkaYy7IDBV9aokrwX2ovXkXVxVNyxaZJIkSdqssRO5TgE/727/coFjkSRJ0iyMu47c3ZKcDKwH/gP4BrA+yZuT3H0xA5QkSdJo4/bIvQ04GHg+ty9Dsi/wt8C9gD9Z+NAkSZK0OePOWv194HlV9d6q+n63vRc4AnjmODtIcq8kX0lyQ5I9u7LvJjm32/bvyvZMsjbJBUnWdGXbJzkryZeSHDWwz5O6uu9KsmI2By5JktR34yZyNwJXjSi/Crh5zH3cDBwCfGigbH1VHdBt53Vlr7DgLhMAABGnSURBVAUOB54MnNCVHQF8rKpWA09NskuSRwG7VtWaLo5DxoxDkiRpizBuIvcW4Ngk200VdLdf3T02o6q6rarWDRXvmOS8JKd3y5sA7FZVl1XVBuDW7nVWA2d3j58D7DNU9mnaUK8kSdJdxrTnyCX52FDRAcBVSb7R3X9E9/x7zuP196uqnyY5AngF8HIgA49vBHYCVgLXDZTt3JX9cKhs+BhWdvUG7T6PeCVJkpaNzU12+OnQ/Q8P3b98vi9eVVOvcSbw3u72poEqOwIbaInaDrRru+5IuyzYSm6/zutUvWFHAcfON05JkqTlaNpErqqet5gvnGRbIFV1C7AfcGn30DVJVtGSt22r6uYk5wMHAacBB9IuGXY1cCRwBm1G7QXc2cnAqUNluwNrF/JYJEmSJmG2CwLPS5KPA48BHgq8HXhJkhuAm2gTHACOAU4HVtCGWwFOAU5P8kLgzO5cu3VJ1idZC3wfOH749apqIy0hHIxhoQ9LkiRpIsZK5JLsBBxH6w27D0OTJKrqPuPsp6qeMlT07hF1LgbWDJVdDxw6ou7R47yuJEnSlmjcHrnTgP+Hlnj9hHapLkmSJE3QuIncAcD+VXXRIsYiSZKkWRh3HbnLZlFXkiRJS2Dc5OwlwN8meZSXwpIkSVoexh1avRTYDrgI7jzzs6pM7iRJkpbYuInc+2iL7h6Jkx0kSZKWhXETuX2Ax1bVNxczGEmSJI1v3HPkLub2y2FJkiRpGRg3kTsGeFOSg5LsmmTnwW0xA5QkSdJo4w6tfqr7eTZ3PD8u3X0nO0iSJC2xcRO5Axc1CkmSJM3aWIlcVZ232IFIkiRpdsZK5JLsvbnHvXSXJEnS0ht3aPVC2rlwgysBD54r5zlykiRJS2zcRG6PofvbAL8JvAp4xYJGJEmSpLGMe47cD0YUX5rkOuBY4F8WNCpJkiTNaNx15KZzOfDohQhEkiRJszPuZIfhRX8D3Bc4DrhkgWOSJEnSGMY9R+5a7ji5AVoy91/A/1rQiCRJkjSWuS4IvAlYB1xaVbctbEiSJEkahwsCS5Ik9dRmE7kR58aNVFXrFyYcSZIkjWumHrlR58YNqzH2I0mSpAU2UwI2fG7coN8FXgJ4jpwkSdIEbDaRG3VuXHfd1ROBxwFvB45fnNAkSZK0OWMvCJxkjyRnAF8B1gN7VdWRVbVu0aKTJEnStGZM5JLcO8mbge8AuwH7VtX/qqrLFj06SZIkTWuziVySVwKXAfsDT6uqx1fVhUsSmSRJkjZrpskOJwA3A1cCL0ryolGVquqpCx2YJEmSNm+mRO40Zl5+RJIkSRMw06zVw5coDkmSJM3S2LNWJUmStLyYyEmSJPWUiZwkSVJPmchJkiT1lImcJElST5nISZIk9ZSJnCRJUk+ZyEmSJPXUkiVySe6V5CtJbkiyZ1f2rCTnJ/lckt26sj2TrE1yQZI1Xdn2Sc5K8qUkRw3s86Su7ruSrFiqY5EkSVoOlrJH7mbgEOBDAEm2Bo4CDgBeB7ysq/da4HDgybRrvQIcAXysqlYDT02yS5JHAbtW1Rrgqm7fkiRJdxlLlshV1W1VtW6g6KHAt6rqF8A5wGO68t2q6rKq2gDcmmQ7YDVwdvf4OcA+Q2WfBvYdfs0kK5OsGtyA3Rf40CRJkiZis9daXWQrgesAqmpT10MHkIE6G4GdBut2ZTt3ZT8cKht2FHDswoYtSZK0PExyssNGYAeAJFsBt3Xlmwbq7AhsGKw7Q9mwk4E9hrY1C3YEkiRJEzTJHrnvAXsl2QbYD7ioK7+mGwLdCGxbVTcnOR84CDgNOBD4J+Bq4EjgDOBg4ILhF6iqjd1+fiXJcLU72WOPK+ZyPFuMyy9fNekQJEnSGJY0kUvycdq5cA8F/gF4K3AecBPw7K7aMcDpwArgFV3ZKcDpSV4InNmda7cuyfoka4HvA8cv2YFIkiQtA0uayFXVU0YUv3+ozsUMDX9W1fXAoSP2d/SCBihJktQjLggsSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPTXRRC7JqiTXJDm323ZM8qwk5yf5XJLdunp7Jlmb5IIka7qy7ZOcleRLSY6a5HFIkiRNwnLokTunqg6oqgOAG4GjgAOA1wEv6+q8FjgceDJwQld2BPCxqloNPDXJLksYsyRJ0sQth0TucV1v22uAhwLfqqpfAOcAj+nq7FZVl1XVBuDWJNsBq4Gzu8fPAfYZ3nGSlV2v3682YPdFPh5JkqQlsfWEX/9q4CHAzcC7gEOB6wCqalOSqfgy8JyNwE7Ayqm6XdnOI/Z/FHDswoctSZI0eRPtkauqW6rqxqraBHyElljuAJBkK+C2ruqmgaftCGygJW87DJUNOxnYY2hbs8CHIUmSNBGTnuxwr4G7+wGXAHsl2QbYH7ioe+yabmh0JbBtVd0MnA8c1D1+IHDh8P6ramNVXTG4AVcu0uFIkiQtqUkPra5JcjxwE/Ad4J9pw6jndWXP7uodA5wOrABe0ZWdApye5IXAmVW1bikDlyRJmrSJJnJV9SngU0PF7++2wXoXMzQkWlXX086pkyRJuktaDrNWJUmSNAcmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUU1tPOgBJkvpujz2umHQIE3X55asmHcJdlj1ykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTJnKSJEk9ZSInSZLUUyZykiRJPWUiJ0mS1FMmcpIkST1lIidJktRTXqJLC+6ufKkaL1MjSVpK9shJkiT1lImcJElST5nISZIk9ZSJnCRJUk+ZyEmSJPWUiZwkSVJPufyIpC3GXXnpG3D5G+muyB45SZKknjKRkyRJ6qneD60mOQnYF7gUeH5V/XLCIUmSpFnwtIhVc35ur3vkkjwK2LWq1gBXAYdMOCRJkqQl0/ceudXA2d3tT9MSuY9OPZhkJbBy6DkPALjyyiun3emtt07/2F3BFVfM7/l35faz7ebH9puf+baf5s7P3vyeb/tN/9hAvrJi1OOpqgUPaKkkeSXwjar6RJKHA0dW1QsHHj8OOHZS8UmSJC2QNVX1xeHCvvfIbQR26G7vCGwYevxk4NShsm2BBwLfA5bj+XS7A2uBNcBd+yvK3Nh+c2fbzY/tNz+239zZdvOz3NtvBXBf4GujHux7Inc+cCRwBnAwcMHgg1W1kZbsDfvu4oc2N0mmbl5ZVVdMMJResv3mzrabH9tvfmy/ubPt5qcn7XfZdA/0erJDVX0dWJ9kLbAK+MRkI5IkSVo6fe+Ro6qOnnQMkiRJk9DrHjlJkqS7MhO55Wcj8BpGn9unmdl+c2fbzY/tNz+239zZdvPT6/br9fIjkiRJd2X2yEmSJPWUiZwkSVJPmchJkiT1lImcJElST5nILQNJ7pNk2+723kkeNumY+iTJzkkOTvIHSZ6U5N6TjqnPkuw16Rj6IslOSXYYKnvgpOLpG//2LawkfzzpGPomycok+yTZZdKxzJWzVicsyfHAauDHwLXAA4Abge9V1bGTjK0PkrwEeCrwr8B1tGvuHgh8sqpOnmRsfZXk7Ko6eNJxLHdJjqZ99m4FLgWOrKpbkpxTVY+fbHTLn3/75ifJGcNFwOOA86rqjyYQUm8kOaWqjkjydOAVtMt97g2cXlXvmGx0s2ciN2FJzq+q30myNXAJ8OCqqiRrq2rNpONb7qZrpyRfqKrHTSKmvkjyQ+58gegAe1bVThMIqVeSfKmqVne3nwq8BHgO7Z+BidwM/Ns3P0neCtwf+Dvg+7QRtlOB51TVDycY2rI39WUryXnAIVV1ffc5XFtV+046vtnq/SW6tgC3AVTVbUneWrdn1rdNMKY+WZfkucDZtB65HYCDgJ9ONKp++Bnw+Kr6+WBhks9OKJ6+WTF1o6o+luRS4IPAbpMLqVf82zcPVfXiJPcFjgbuSUvobjKJG8uvJ3khcO+quh5+9TmccFhzYyI3ef+YZEVV/bKq/g6gO2fkXyYcV188GzgCOBlYCWygdZM/e5JB9cQLgG2Anw+Vv2gCsfTRW5L8xtQ/zqq6uBuqeeWE4+oL//bNU1VdDfxlkt2AvwJunnBIffE33c/XJ9mhqn6W5F709LPn0OoykeRuwE7Ahqq6ZdLxSJKk5c9ZqxOWZHWSc4GPA28BPp7k80n2m2xk/Zakl9+slgPbbn5sv/mx/ebH9pu7vradQ6uTdxLwe1X1q4v1JlkJfAIwmZvBiJlb0E7Yf9RSx9I3tt382H7zY/vNj+03d1ta25nITd4vaed2bRwo2xHYNJlwemdf4PHcsb0CnD6ZcHrFtpsf229+bL/5sf3mbotqOxO5yXsB8IbuZNXQPlg/Af50olH1x+uB66vq2sHCJG+eUDx9YtvNj+03P7bf/Nh+c7dFtZ2THSRJknrKHrkJS3If4OW0VaW3pq2hdBFwYlX9ZJKx9cFA+z2Gtq6X7Tcm225+bL/5sf3mx/abuy2u7arKbYIbbSHbJ3B772hol5g6e9Kx9WHr2u/xtp9tZ/v1a7P9bD/bbmE2lx+ZvHvSro1XAN3PtV25ZnZP4Au235zYdvNj+82P7Tc/tt/cbVFt59Dq5L0ZODfJt7j9ou8P68o1M9tv7my7+bH95sf2mx/bb+62qLYzkZu8D9Fmqt4X+DpwLfA94KmTDKpHbL+5s+3mx/abH9tvfmy/udui2s6h1ck7HXg0cG/gdcBtVXUb8OKJRtUftt/c2XbzY/vNj+03P7bf3G1RbWeP3OTdr6oOA0hyCvDuJK+bcEx9YvvNnW03P7bf/Nh+82P7zd0W1Xb2yE3etknuAVBV/wUcAvwZPb1UyATYfnNn282P7Tc/tt/82H5zt0W1nQsCT1iSRwBX18AK00kCPK2qPjK5yPrB9ps7225+bL/5sf3mx/abuy2t7UzkJEmSesqhVUmSpJ4ykZMkSeopEzlJkqSeMpGTpAFJ3pPk60m2HSp/QpJbk/zOpGKTpGEmcpJ0Ry+mLRR67FRBkh2AdwKvr6rzF/LFhhNGSZoNEzlJGlBVG4HnAS9N8tiu+O+ADcDbk7w/yYZu+2SSh0w9N8mDknw0yY+T3JjkoiSHDO4/yRVJjkvyziQbgfcu1bFJ2vKYyEnSkKr6HPA24LQkzwQOA54LfBb4ObA/sC9wNfC5qcVFge2BfwGeSFtc9MPAmUn2HHqJvwC+A+wDvHJxj0bSlsx15CRphCTbAf8OPAR4OfBT4BXAQ6v7w5lkBXAN8L+r6oPT7OfLwCeq6oTu/hXAf1bVUxb9ICRt8eyRk6QRqupm4A3ALcAbgccAewDXJ7khyQ3AdcBOwIMAktwzyUlJLu6GXm+g9br9xtDuL1yq45C0Zdt60gFI0jJ2G7CpqjYl2Qr4OvAHI+qt736+Afhd4Gjge8BNwGnA8ISGGxcnXEl3NSZykjSei4A/BK7tJkSMsh9wWlV9GCDJ3Wm9dd9dmhAl3dU4tCpJ43kv8BPgo0n2T7JHkscleePAzNXvAk9Psnd3Ye73AHefVMCStnwmcpI0hqq6CXgc8H3gn2mzTt9NO0duQ1ftL2iTH9bSZq9+ubstSYvCWauSJEk9ZY+cJElST5nISZIk9ZSJnCRJUk+ZyEmSJPWUiZwkSVJPmchJkiT1lImcJElST5nISZIk9ZSJnCRJUk/9X4bUCz2yb2AeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a bar plot of Messages sent per year using Seaborn's countplot() method. I choose to plot the bars in the order\n",
    "# in which they appear in the data rather than order them by decreasing counts.\n",
    "\n",
    "# Set the figure size:\n",
    "plt.figure(figsize=(10,6))\n",
    "# Set the context for the notebook (for font scale primarily):\n",
    "sns.set_context(\"notebook\", font_scale=0.75, rc={\"lines.linewidth\": 1.5})\n",
    "# Generate the bar plot, and make each bar the same colour:\n",
    "sns.countplot(data = sms_text_data, x=\"year\", color = 'blue')\n",
    "# Make the x-axis labels easier to read, and make sure the axis labels are informative and readable:\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel(\"Year\", fontsize=14)\n",
    "plt.ylabel(\"Number per Year\", fontsize=14)\n",
    "plt.title(\"Number of Text Messages Sent Per Year\", fontsize=18)\n",
    "# Display the plot:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well over half of the text messages in the data set were sent in 2011; the year with the next most messages sent was 2003. Upon seeing the results of this plot, I decided that it would be interesting to see how (and if) common text message topics changed in the eight years between 2003 and 2011. I further decided to limit myself to considering messages sent from people from Singapore each of those years; this is because Singapore accounts for about 70% of all text messages in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section4\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Text Pre-Processing </h3> <br>\n",
    "Text message data is noisy. It is common for message senders to use numbers in place of letters/words, to use symbols in place of letters/words, to include standard and non-standard punctuation, and to include emoji and other text-based art in the body of their messages. URLs may also be included. Before doing anything else, I must remove some noise from the text messages. For my purposes, this noise removal will include standard punctuation marks, standard keyboard symbols, and the special character ┾. I will also remove the token &lt; DECIMAL &gt;, as it appears a few times in the corpus and it does not appear to convey any linguistical meaning.  I chose not to remove special characters like ü because they could be important to non-English words found in the text messages; I also chose not to remove numbers, because they could be taking the place of letters and/or words in the messages (e.g., 2 instead of \"to\"). <br>\n",
    "It is also common to normalize text before engaging in word count, TF-IDF, word embedding, and/or n-gram analysis. For my purposes, text normalization includes converting all characters to lowercase, tokenizing the text messages (i.e., breaking them up into individual words), removing stopwords (i.e., removing common words which are important for sentence structure but not for determining topics or overall sentiment) and lemmatizing the remaining words (i.e., casting words into their root forms). <br>\n",
    "Noise removal and text normalization are conducted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few noise removal functions to make noise removal on Messages in the DataFrame easier:\n",
    "\n",
    "# Replace punctuation marks and standard symbols with a single space:\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r',|\\.|\\:|;|-|/|&|!|\\?|\\(|\\)|\\+|@|<|>|#|~|=|\\$|\\*|[|]|{|}',' ',text)\n",
    "\n",
    "# Replace apostrophes with a single space:\n",
    "def replace_apostrophe(text):\n",
    "    return re.sub(r\"'\",' ',text)\n",
    "\n",
    "# Replace the special character ┾ with a single space:\n",
    "def replace_crossmark(text):\n",
    "    return re.sub(r\"┾\",' ',text)\n",
    "\n",
    "# Implementing some of the above functions may introduce multiple whitespaces ('  ' or '   ', etc.)\n",
    "# Replace any instances of multiple whitespaces with a single whitespace:\n",
    "def remove_xtra_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "# The following code was written by the curriculum developers at Codecademy in their lesson on word lemmatization. \n",
    "# I have not changed any of the original code they wrote. The purpose of the code is to determine which part of \n",
    "# speech a word most probably belongs to -- noun, verb, adjective, adverb -- and to return that part of speech.\n",
    "# The result of that code is in turn handed off to a word lemmatizer, so it \"knows\" which part of speech a word\n",
    "# most likely belongs to.\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos()==\"n\"])\n",
    "    pos_counts[\"v\"] = len([item for item in probable_part_of_speech if item.pos()==\"v\"])\n",
    "    pos_counts[\"a\"] = len([item for item in probable_part_of_speech if item.pos()==\"a\"])\n",
    "    pos_counts[\"r\"] = len([item for item in probable_part_of_speech if item.pos()==\"r\"])\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Noise Removal </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use some of the above functions to remove noise from the Messages, and to begin normalizing them:\n",
    "\n",
    "# The specific pattern of text <DECIMAL> was observed in a number of text messages. Replace it with a single space:\n",
    "sms_text_data.Message = sms_text_data.Message.replace('<DECIMAL>', ' ', regex = False)\n",
    "\n",
    "# Remove apostrophes and punctuation/certain symbols from the Messages:\n",
    "sms_text_data.Message = sms_text_data.Message.apply(lambda x: replace_apostrophe(x))\n",
    "sms_text_data.Message = sms_text_data.Message.apply(lambda x: replace_crossmark(x))\n",
    "sms_text_data.Message = sms_text_data.Message.apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "# Remove any extra whitespace inadvertantly added to the Messages:\n",
    "sms_text_data.Message = sms_text_data.Message.apply(lambda x: remove_xtra_whitespace(x))\n",
    "\n",
    "# Convert all characters to lowercase:\n",
    "sms_text_data.Message = sms_text_data.Message.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Tokenization </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break each text message up into individual words using the NLTK method word_tokenize:\n",
    "sms_text_data['Message_tokenized'] = sms_text_data.Message.apply(lambda text_message: word_tokenize(text_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Stopword Removal </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of common English stopwords for stopword removal:\n",
    "eng_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              [bugis, oso, near, wat]\n",
      "1    [go, jurong, point, crazy, available, bugis, n...\n",
      "2                    [dunno, lets, go, learn, pilates]\n",
      "3    [den, weekdays, got, special, price, haiz, can...\n",
      "4                                    [meet, lunch, la]\n",
      "Name: Message_tokenized_nostop, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords from the text Messages.\n",
    "# In retrospect, this probably would have been easier to do if the tokenized messages were in a list rather than a\n",
    "# DataFrame column, but that is a lesson that I will carry into my next NLP project.\n",
    "\n",
    "# Make a new column in the DataFrame which contains, for now, empty lists:\n",
    "sms_text_data['Message_tokenized_nostop'] = [list() for i in range(len(sms_text_data['Message_tokenized']))]\n",
    "\n",
    "# Loop over each row of the DataFrame, gather up the tokenized text, and loop through each token to see whether \n",
    "# it is a stopword. If it is NOT a stopword, add it to the list in Message_tokenized_nostop for that particular \n",
    "# row; otherwise, leave the word out of said list:\n",
    "# Begin loop:\n",
    "for i in range(len(sms_text_data['Message_tokenized'])):\n",
    "    # Pluck off the list of tokens corresponding to this particular row:\n",
    "    thistext = sms_text_data.Message_tokenized.iloc[i]\n",
    "    # Loop over tokens in the current list of tokens:\n",
    "    for word in thistext:\n",
    "        # If the token in question is not in the list of English stopwords, add it to the list of tokens in\n",
    "        # Message_tokenized_nostop for this particular row.\n",
    "        if not word in eng_stopwords:\n",
    "            sms_text_data.Message_tokenized_nostop.iloc[i].append(word)\n",
    "\n",
    "# Print out the header for the new column Message_tokenized_nostop to make sure that nothing blatantly obvious has gone\n",
    "# wrong (with the first 5 records at least):\n",
    "print(sms_text_data.Message_tokenized_nostop.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent that many of these messages contain a mix of English and non-English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Word Lemmatization </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an object of the WordNetLemmatizer() class:\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              [bugis, oso, near, wat]\n",
      "1    [go, jurong, point, crazy, available, bugis, n...\n",
      "2                      [dunno, let, go, learn, pilate]\n",
      "3    [den, weekday, get, special, price, haiz, cant...\n",
      "4                                    [meet, lunch, la]\n",
      "Name: Msg_token_nostop_lemmmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the remaining words in the text messages.\n",
    "# In retrospect, this also probably would have been easier to do if the tokenized messages with stopwords removed were\n",
    "# in a list rather than a DataFrame column, but that is another lesson that I will carry into my next NLP project.\n",
    "\n",
    "# Make a new column in the DataFrame which contains, for now, empty lists:\n",
    "sms_text_data['Msg_token_nostop_lemmmed'] = [list() for i in range(len(sms_text_data['Message_tokenized_nostop']))]\n",
    "\n",
    "# Loop over each row, gather up the tokenized text, go through the tokenized text word by word to lemmatize it (after\n",
    "# determining which part of speech the word most probably belongs to), and add the lemmatized word to the list in\n",
    "# Msg_token_nostop_lemmed for that particular row:\n",
    "# Begin loop:\n",
    "for i in range(len(sms_text_data['Message_tokenized_nostop'])):\n",
    "    # Pluck off the list of tokens corresponding to this particular row:\n",
    "    thistext = sms_text_data.Message_tokenized_nostop.iloc[i]\n",
    "    # Loop over tokens in the current list of tokens: \n",
    "    for word in thistext:\n",
    "        # Lemmatize the token after first determining its likely part of speech:\n",
    "        lemmatized = lemmatizer.lemmatize(word, get_part_of_speech(word))\n",
    "        # Add the lemmatized token to the list of tokens in Msg_token_nostop_lemmed for this particular row:\n",
    "        sms_text_data.Msg_token_nostop_lemmmed.iloc[i].append(lemmatized)\n",
    "\n",
    "# Print out the header for the new column Msg_token_nostop_lemmmed to make sure that nothing blatantly obvious has gone\n",
    "# wrong (with the first 5 records at least):\n",
    "print(sms_text_data.Msg_token_nostop_lemmmed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section5\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Average Text Lengths Per Country </h3> <br>\n",
    "Having cleaned the data, removed noise from the text message data, and normalized the text message data, it is time to begin answering some of my central questions about the data set. I will begin with the first question I posed, because it is arguably the easiest question to answer and is the sort of question I have the most experience answering. That question is, \"Text message senders from which countries send the longest messages?\" <br>\n",
    "To answer this question, I will simply calculate the average text message length as broken down by country the sender is from. I will then report the top 5 countries in terms of average message length. I will, however, note from the onset that my answer to the above question will suffer a little bit from small number statistics, as over half of the countries have fewer than 100 text messages to average over and some have fewer than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      country      length\n",
      "3      Canada  103.782828\n",
      "27  Sri Lanka   94.493792\n",
      "14  Macedonia   92.200000\n",
      "6       Ghana   73.500000\n",
      "24  Singapore   55.996134\n"
     ]
    }
   ],
   "source": [
    "# Group records by country and calculate the average length of Messages:\n",
    "avg_text_lengths = sms_text_data.groupby(['country']).length.mean().reset_index()\n",
    "\n",
    "# Sort the DataFrame by average length of Messages:\n",
    "avg_text_lengths.sort_values(by=['length'], ascending = False, inplace = True)\n",
    "\n",
    "# Print out the names of the five countries with the biggest average length of Messages:\n",
    "print(avg_text_lengths.nlargest(5,'length'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People from Canada sent the longest text messages (of messages in this particular corpus), followed by people from Sri Lanka, Macedonia, Ghana, and Singapore. I caution the reader that the averages from Sri Lanka, Macedonia, and Ghana almost certainly suffer from small number statistics, so the averages calculated here may not be truly representative of the lengths of messages that people from Sri Lanka/Macedonia/Ghana send when in Singapore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section6\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Important Words in Messages: TF and TF-IDF Analysis </h3> <br>\n",
    "The next four sections are aimed at answering the following two central questions of this project: \"What are some of the most common topics of text messages sent by people from Singapore, India, and the United States? How are they similar, and how are they different?\" and \"How (if at all) did the common topics of text messages sent by people from Singapore change between 2003 and 2011?\" I will be exploring three main avenues of addressing these questions. I will begin by seeing what I can learn by conducting term frequency (TF) and term frequency-inverse document frequency (TF-IDF) analysis on text messages from users from Singapore, India, and the United States. \"Term frequency\" simply refers to how often a term/word appears in a corpus of documents; it is a count of how often the word appears in the corpus. \"Term frequency-inverse document frequency\" is a measure of how important a word is to a particular document, given how often it appears in the document and over the corpus of documents. Words which appear in many documents in the corpus are \"penalized\" in TF-IDF analysis, whereas words that appear infrequently are not; the assumption is that if a word appears in one document but not in other documents in the corpus, it must be of particular importance to that document. The goal of looking at TF and TF-IDF analysis of texts sent by people from Singapore, India, and the United States will be to get a sense of the most common and the most relevant words found in those texts; this should give me some overall sense of what people are most commonly talking about in their text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out text messages from users from Singapore, India, and the United States into separate DataFrames for\n",
    "# easier analysis:\n",
    "sg_sms_text_data = sms_text_data[sms_text_data.country == 'Singapore']\n",
    "in_sms_text_data = sms_text_data[sms_text_data.country == 'India']\n",
    "us_sms_text_data = sms_text_data[sms_text_data.country == 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from Singapore: </h5> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  count\n",
      "129        u   8283\n",
      "93      haha   6630\n",
      "4         go   4477\n",
      "16       get   3097\n",
      "2518     lol   2818\n",
      "247       ok   1813\n",
      "1454  hahaha   1752\n",
      "263     late   1707\n",
      "120     time   1631\n",
      "153     okay   1608\n",
      "1729      le   1458\n",
      "100    think   1448\n",
      "43      come   1371\n",
      "38      meet   1368\n",
      "64       one   1264\n"
     ]
    }
   ],
   "source": [
    "# Term-Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "sg_corpus = []\n",
    "\n",
    "# Loop over records and, for each record, append each word of the associated message to the list sg_corpus. This\n",
    "# creates a list of all non-stopword, lemmatized words in all text messages sent by users from Singapore:\n",
    "# Begin loop:\n",
    "for i in range(len(sg_sms_text_data['Msg_token_nostop_lemmmed'])):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thistext = sg_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Loop over tokens in the current list:\n",
    "    for word in thistext:\n",
    "        # Append the current token to the master list sg_corpus:\n",
    "        sg_corpus.append(word)\n",
    "\n",
    "# Instantiate a Counter class object from the iterable list sg_corpus:\n",
    "sg_count_of_words = Counter(sg_corpus)\n",
    "# Since objects of the Counter class are a subclass of dictionaries, convert the Counter object sg_count_of_words\n",
    "# to a Pandas DataFrame:\n",
    "sg_word_counts = pd.DataFrame.from_dict(sg_count_of_words, orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "sg_word_counts.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "sg_word_counts.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly used words:\n",
    "print(sg_word_counts.nlargest(15,'count'))\n",
    "\n",
    "# I realized after completing the project that simply using the Counter method most_common() would have been faster and\n",
    "# perhaps more efficient than what I did above. This is another lesson that I will be taking into my next NLP project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the most common words found in text messages sent by people from Singapore are haha, lol, and hahaha; these could be written in reaction to an event and/or previous text message, or to show that a part of the message being sent is amusing/not that serious. Other commonly found words such as \"come,\" \"meet,\" \"late,\" \"go,\" and \"get\" indicate that messages often plan meetings, warn the receiver that the sender will be late to a meeting, or ask if the receiver would like to meet up to do something. I would expect such messages to be common in a corpus of text messages. Also appearing in the list of common words is \"ok,\" which I would also expect to commonly see in a corpus of text messages as senders acknowledge the receipt of a previous message or communicate their assent to some question being asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency-Inverse Document Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "sing_corpus = []\n",
    "\n",
    "# Loop over records and, for each record, join the list of non-stopword, lemmatized words into one string, then append\n",
    "# that string to the list of text message strings to be stored in sing_corpus:\n",
    "\n",
    "# Begin loop:\n",
    "for i in range(len(sg_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thislist = sg_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Join the tokens in the current list into one string, separating them by commas:\n",
    "    thisstring = ', '.join(thislist) \n",
    "    # Append the current string to the list sing_corpus:\n",
    "    sing_corpus.append(thisstring)\n",
    "\n",
    "# Instantiate a TfidVectorizer object, making sure that inverse document frequency will be calculated:\n",
    "sing_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "# Fit the sing_vectorizer object to the corpus contained in sing_corpus:\n",
    "sing_tfidf_vectors = sing_vectorizer.fit_transform(sing_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object sing_tfidf_vector is essentially a collection of vectors, with each vector corresponding to one text message and each element of each vector corresponding to a word found in the corpus of text messages from users in Sinagpore. Given that there are over 30,000 individual text messages in the corpus, and that the number of words found in the entire corpus is easily in the tens of thousands, evaluating each vector of the collection of vectors sing_tfidf_vectors individually is  not feasible. Instead, what I will do is loop over vectors, find the non-zero elements of that vector, and append the resulting words to a list. Then, I will count how many times a word appeared in the list of \"important\" words, and look for the top 15 \"important\" words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  count\n",
      "89      haha   4722\n",
      "10        go   4036\n",
      "9        get   2861\n",
      "2490     lol   2411\n",
      "238       ok   1745\n",
      "251     late   1646\n",
      "116     time   1572\n",
      "145     okay   1518\n",
      "1430  hahaha   1432\n",
      "1705      le   1406\n",
      "98     think   1379\n",
      "38      come   1321\n",
      "36      meet   1306\n",
      "74       one   1154\n",
      "214       ur   1133\n"
     ]
    }
   ],
   "source": [
    "# Begin loop:\n",
    "for i in range(len(sg_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the tf-idf vector corresponding to the current record, i.e. text message:\n",
    "    tfidf_vector = sing_tfidf_vectors[i]\n",
    "    # Place tf-idf values in a Pandas DataFrame, setting the indices of the DataFrame to the words found in the\n",
    "    # corpus of text messages:\n",
    "    tfidf_results = pd.DataFrame(tfidf_vector.T.todense(), index = sing_vectorizer.get_feature_names(), \\\n",
    "                                 columns = [\"TF-IDF Score\"])\n",
    "    # Keep only those records in tfidf_results which have a non-zero tf-idf score:\n",
    "    nonzero_words = tfidf_results[tfidf_results['TF-IDF Score'] > 0.0]\n",
    "    # If this is the first vector, make the DataFrame of non-zero tf-idf words equal to the current DataFrame \n",
    "    # nonzero_words:\n",
    "    if i == 0:\n",
    "        sing_top_tfidf_scores = nonzero_words\n",
    "    # Otherwise, append the current DataFrame nonzero_words to the already extant DataFrame sing_top_tfidf_scores:\n",
    "    else:\n",
    "        sing_top_tfidf_scores = sing_top_tfidf_scores.append(nonzero_words)\n",
    "\n",
    "# Convert the DataFrame sing_top_tfidf_words to a list:\n",
    "sing_top_tfidf_words = sing_top_tfidf_scores.index[sing_top_tfidf_scores['TF-IDF Score'] > 0.0].to_list()\n",
    "\n",
    "# Instantiate a Counter class from the iterable list sing_top_tfidf_words:\n",
    "sing_count_of_top_tfidf_words = Counter(sing_top_tfidf_words)\n",
    "# Convert the Counter object sing_count_of_top_tfidf_words to a Pandas DataFrame:\n",
    "sing_top_tfidf_word_counts = pd.DataFrame.from_dict(sing_count_of_top_tfidf_words, orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "sing_top_tfidf_word_counts.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "sing_top_tfidf_word_counts.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly occuring important words in text messages from users from Singapore:\n",
    "print(sing_top_tfidf_word_counts.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these \"most important\" words are also found in the list of most common words, which does not surprise me given the nature of what I did to arrive at the list of \"most important\" words (basically, a modified TF analysis) and words themselves. It is interesting to note that, of words that appeared in both the TF and TF-IDF \"top 15\" lists, the order has shifted a bit, perhaps indicating how important overall a word tends to be to text messages as a whole. For the most part, these top 15 \"most important\" words are those one would expect to be important words in text messages: \"late,\" \"time,\" \"come,\" \"meet,\" \"go,\" \"get,\" \"time.\" These words again give a sense that the messages are setting up meetings, letting the receiver know that a sender will be late for a meeting, or asking the receiver if they would like to go do something. I did find it interesting that the Malay word \"le\" appears in both the top 15 \"most common\" words list and the top 15 \"most important\" words list; Google tells me that \"le\" is used either to change a verb into a command or to soften a verb's tone in Malay slang. The latter seems like an important idea to convey in a text message (and frankly English could use a similar term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from India: </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  count\n",
      "48      u   1190\n",
      "0       k    403\n",
      "83   come    393\n",
      "578     2    338\n",
      "33     ur    323\n",
      "527  call    323\n",
      "214   get    302\n",
      "529    ok    301\n",
      "297    go    300\n",
      "194     r    293\n",
      "45    hey    247\n",
      "487    ``    239\n",
      "128     n    238\n",
      "397   day    237\n",
      "412  good    232\n"
     ]
    }
   ],
   "source": [
    "# Term-Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "in_corpus = []\n",
    "\n",
    "# Loop over records and, for each record, append each word of the associated message to the list in_corpus. This\n",
    "# creates a list of all non-stopword, lemmatized words in all text messages sent by users from India:\n",
    "# Begin loop:\n",
    "for i in range(len(in_sms_text_data['Msg_token_nostop_lemmmed'])):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thistext = in_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Loop over tokens in the current list:\n",
    "    for word in thistext:\n",
    "        # Append the current token to the master list in_corpus:\n",
    "        in_corpus.append(word)\n",
    "\n",
    "# Instantiate a Counter class object from the iterable list in_corpus:\n",
    "in_count_of_words = Counter(in_corpus)\n",
    "# Convert the Counter object in_count_of_words to a Pandas DataFrame:\n",
    "in_word_counts = pd.DataFrame.from_dict(in_count_of_words, orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "in_word_counts.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "in_word_counts.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly used words\n",
    "print(in_word_counts.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list of the top 15 \"most common\" words found in texts sent by people from India is markedly different than the list of the top 15 \"most common\" words found in texts sent by people from Singapore. In fact, the only five words the two lists have in common are \"u,\" \"come,\" \"go,\" \"get,\" and \"ok.\" It is striking to see how many number and single-letter abbreviations for words appear in the top 15 \"most common\" words found in texts sent by people from India; clearly, these text message senders value brevity in typing. If there are character limits on text messages, this would make a great deal of sense to me; however, I do not know if this is the case. Also, I found the appearance of \"call\" to be particularly interesting. (I also note that the empty string \\`\\` appears in the list of most common \"words.\" Given that I have not set any tokens to an empty string, I do not know why it is appearing in the list of most commonly found words.) Given the above top 15 \"most common\" words list, one could imagine that some of the most commonly sent text messages wish someone a good day, or ask them to call the message sender, in addition to messages which set up meetings or ask if the receiver wants to go do something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  count\n",
      "79   come    372\n",
      "511  call    302\n",
      "514    ok    289\n",
      "206   get    286\n",
      "284    go    283\n",
      "31     ur    248\n",
      "44    hey    247\n",
      "394  good    220\n",
      "136    da    204\n",
      "383   day    200\n",
      "602    hi    184\n",
      "76   send    170\n",
      "369  dear    160\n",
      "46   tell    158\n",
      "59     na    151\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "india_corpus = []\n",
    "\n",
    "# Loop over records and, for each record, join the list of non-stopword, lemmatized words into one string, then append\n",
    "# that string to the list of text message strings to be stored in india_corpus:\n",
    "# Begin loop:\n",
    "for i in range(len(in_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thislist = in_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Join the tokens in the current list into one string, separating them by commas:\n",
    "    thisstring = ', '.join(thislist) \n",
    "    # Append the current string to the list india_corpus:\n",
    "    india_corpus.append(thisstring)\n",
    "\n",
    "# Instantiate a TfidVectorizer object, making sure that inverse document frequency will be calculated:    \n",
    "india_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "# Fit the india_vectorizer object to the corpus contained in india_corpus:\n",
    "india_tfidf_vectors = india_vectorizer.fit_transform(india_corpus)\n",
    "\n",
    "# Begin loop over vectors:\n",
    "for i in range(len(in_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the tf-idf vector corresponding to the current record, i.e. text message:\n",
    "    tfidf_vector = india_tfidf_vectors[i]\n",
    "    # Place tf-idf values in a Pandas DataFrame, setting the indices of the DataFrame to the words found in the\n",
    "    # corpus of text messages:\n",
    "    tfidf_results = pd.DataFrame(tfidf_vector.T.todense(), index = india_vectorizer.get_feature_names(), \\\n",
    "                                 columns = [\"TF-IDF Score\"])\n",
    "    # Keep only those records in tfidf_results which have a non-zero tf-idf score:\n",
    "    nonzero_words = tfidf_results[tfidf_results['TF-IDF Score'] > 0.0]\n",
    "    # If this is the first vector, make the DataFrame of non-zero tf-idf words equal to the current DataFrame \n",
    "    # nonzero_words:\n",
    "    if i == 0:\n",
    "        india_top_tfidf_scores = nonzero_words\n",
    "    # Otherwise, append the current DataFrame nonzero_words to the already extant DataFrame india_top_tfidf_scores:    \n",
    "    else:\n",
    "        india_top_tfidf_scores = india_top_tfidf_scores.append(nonzero_words)\n",
    "\n",
    "# Convert the DataFrame india_top_tfidf_words to a list:\n",
    "india_top_tfidf_words = india_top_tfidf_scores.index[india_top_tfidf_scores['TF-IDF Score'] > 0.0].to_list()\n",
    "\n",
    "# Instantiate a Counter class from the iterable list india_top_tfidf_words:\n",
    "india_count_of_top_tfidf_words = Counter(india_top_tfidf_words)\n",
    "# Convert the Counter object india_count_of_top_ifidf_words to a Pandas DataFrame:\n",
    "india_top_tfidf_word_counts = pd.DataFrame.from_dict(india_count_of_top_tfidf_words, orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "india_top_tfidf_word_counts.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "india_top_tfidf_word_counts.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly occuring important words in text messages from users from India:\n",
    "print(india_top_tfidf_word_counts.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit over half or so of the words that were in the top 15 \"most common\" words list also appear in the top 15 \"most important\" words list, but all of the single letter/number word abbreviations (and the empty string \\`\\`) are gone. \"Call\" is one of the most commonly found \"important words\" in a text message, which I find to be very interesting. I also find the appearance of \"dear,\" \"tell,\" and \"send\" to be interesting. My interpretation of this list of words is that common text messages from people from India are asking for the receiver to call them and/or telling the receiver to call someone, to wish a loved one a good day, or to ask the receiver to send someone something. While the appearance of \"go,\" \"get,\" and \"come\" indicate that a good deal of messages are sent to plan meetings and/or ask if the receiver would like to go do something, a significant number of messages are meant to quickly inform a loved one how one is on a particular day, to let a loved one know that one is thinking of them, or to set up a time to talk on the phone. That is to say, people from India seem to be using text messages more for brief, personal communications, or to carry on conversations, than do people from Singapore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from the United States: </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  count\n",
      "84      get    602\n",
      "115    know    340\n",
      "418       u    336\n",
      "121      go    315\n",
      "5       lol    278\n",
      "54     like    262\n",
      "68   thanks    256\n",
      "222    want    252\n",
      "215    come    232\n",
      "66     yeah    207\n",
      "345    time    205\n",
      "685      hi    201\n",
      "188   think    192\n",
      "417     see    189\n",
      "407    love    182\n"
     ]
    }
   ],
   "source": [
    "# Term-Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "us_corpus = []\n",
    "\n",
    "# Loop over records and, for each record, append each word of the associated message to the list us_corpus. This\n",
    "# creates a list of all non-stopword, lemmatized words in all texts messages sent by users from the United States:\n",
    "\n",
    "# Begin loop:\n",
    "for i in range(len(us_sms_text_data['Msg_token_nostop_lemmmed'])):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thistext = us_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Loop over tokens in the current list:\n",
    "    for word in thistext:\n",
    "        # Append the current token to the master list us_corpus:\n",
    "        us_corpus.append(word)\n",
    "\n",
    "# Instantiate a Counter class object from the iterable list us_corpus:\n",
    "us_count_of_words = Counter(us_corpus)\n",
    "# Convert the Counter object us_count_of_words to a Pandas DataFrame:\n",
    "us_word_counts = pd.DataFrame.from_dict(us_count_of_words, orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "us_word_counts.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "us_word_counts.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly used words\n",
    "print(us_word_counts.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of top 15 \"most common\" words is more similar in spirit to the list of top 15 \"most common\" words from people from Singapore than it is to the list of top 15 \"most common\" words from people from India. \"Get,\" \"go,\" \"come,\" and \"time\" are all found in the list of top 15 \"most common\" words from people from the United States; these words indicate that meetings and/or get-togethers are being arranged. Similarly to Singapore's top 15 list, the appearance of \"lol\" indicates either the softening of a message or a reaction to a previous message or event. \"Thanks\" indicates acknowledgement of a message, whereas \"hi\" is a very common greeting. I found the appearance of \"think,\" \"love,\" \"know,\" and \"see\" to be very interesting. These words, in conjunction with \"hi\" and \"thanks,\" may be indicating that American text senders are communicating brief messages about their status or carrying on a conversation more than arranging meetings or get-togethers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  count\n",
      "81      get    562\n",
      "111    know    324\n",
      "118      go    298\n",
      "4       lol    271\n",
      "65   thanks    246\n",
      "52     like    243\n",
      "221    want    240\n",
      "208    come    225\n",
      "66     yeah    206\n",
      "667      hi    201\n",
      "347    time    192\n",
      "184   think    186\n",
      "407     see    185\n",
      "191      ok    173\n",
      "396    love    171\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "usa_corpus = []\n",
    "\n",
    "# Loop over records and, for each record, join the list of non-stopword, lemmatized words into one string, then append\n",
    "# that string to the list of text message strings to be stored in sing_corpus:\n",
    "# Begin loop:\n",
    "for i in range(len(us_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thislist = us_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Join the tokens in the current list into one string, separating them by commas:\n",
    "    thisstring = ', '.join(thislist) \n",
    "    # Append the current string to the list usa_corpus:\n",
    "    usa_corpus.append(thisstring)\n",
    "\n",
    "# Instantiate a TfidVectorizer object, making sure that inverse document frequency will be calculated:\n",
    "usa_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "# Fit the usa_vectorizer object to the corpus contained in usa_corpus:\n",
    "usa_tfidf_vectors = usa_vectorizer.fit_transform(usa_corpus)\n",
    "\n",
    "# Begin loop over vectors:\n",
    "for i in range(len(us_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the tf-idf vector corresponding to the current record, i.e. text message:\n",
    "    tfidf_vector = usa_tfidf_vectors[i]\n",
    "    # Place tf-idf values in a Pandas DataFrame, setting the indices of the DataFrame to the words found in the\n",
    "    # corpus of text messages:\n",
    "    tfidf_results = pd.DataFrame(tfidf_vector.T.todense(), index = usa_vectorizer.get_feature_names(), \\\n",
    "                                 columns = [\"TF-IDF Score\"])\n",
    "    # Keep only those records in tfidf_results which have a non-zero tf-idf score:\n",
    "    nonzero_words = tfidf_results[tfidf_results['TF-IDF Score'] > 0.0]\n",
    "    # If this is the first vector, make the DataFrame of non-zero tf-idf words equal to the current DataFrame \n",
    "    # nonzero_words:\n",
    "    if i == 0:\n",
    "        usa_top_tfidf_scores = nonzero_words\n",
    "    # Otherwise, append the current DataFrame nonzero_words to the already extant DataFrame usa_top_tfidf_scores:\n",
    "    else:\n",
    "        usa_top_tfidf_scores = usa_top_tfidf_scores.append(nonzero_words)\n",
    "\n",
    "# Convert the DataFrame usa_top_tfidf_words to a list:\n",
    "usa_top_tfidf_words = usa_top_tfidf_scores.index[usa_top_tfidf_scores['TF-IDF Score'] > 0.0].to_list()\n",
    "\n",
    "# Instantiate a Counter class from the iterable list usa_top_tfidf_words:\n",
    "usa_count_of_top_tfidf_words = Counter(usa_top_tfidf_words)\n",
    "# Convert the Counter object usa_count_of_top_ifidf_words to a Pandas DataFrame:\n",
    "usa_top_tfidf_word_counts = pd.DataFrame.from_dict(usa_count_of_top_tfidf_words, orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "usa_top_tfidf_word_counts.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "usa_top_tfidf_word_counts.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly occuring important words in text messages from users from the United States:\n",
    "print(usa_top_tfidf_word_counts.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of top 15 \"most important\" words is largely the same as the list of top 15 \"most common\" words, save for the disappearance of \"u\" and the appearance of \"like.\" The order in which the words appear between the two lists is largely different. These top 15 \"most important\" words indicate that text messages are being used to carry on conversations, update a loved one about one's current status, and at times to arrange meetings/get-togethers. This is similar to what I inferred from the list of top 15 \"most important\" words in text messages from people in India. It would not surprise me that people from India or the United States, if in Singapore on business or to attend the National University of Singapore, would use text messages for conversation in addition to setting up meetings/get-togethers; after all, loved ones are thousands of miles away and text messaging is a convenient way to communicate if there is a time delay between sender and receiver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section7\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Important Words in Messages: TF and TF-IDF Analysis <br> Comparing and Contrasting Texts from Singapore Eight Years Apart</h3> <br>\n",
    "My third central question for this data set was, \"How (if at all) did the common topics of text messages sent by people from Singapore change between 2003 and 2011?\" I will again engage in TF and TF-IDF analysis as I attempt to answer this question. So in a sense, this analysis repeats what I have done above, except that now I am splitting the corpus of text messages from people from Singapore into two subsets based on the year the messages were sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select texts sent by users from Singapore in 2011, and texts sent by users fron Singapore in 2003:\n",
    "sg_sms_text_data_2011 = sg_sms_text_data[sg_sms_text_data.year == '2011']\n",
    "sg_sms_text_data_2003 = sg_sms_text_data[sg_sms_text_data.year == '2003']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts Sent in 2003 </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  count\n",
      "129     u   3731\n",
      "4      go   1605\n",
      "16    get    969\n",
      "13      e    873\n",
      "41      ü    798\n",
      "210   lor    784\n",
      "225    ur    660\n",
      "247    ok    658\n",
      "190     2    642\n",
      "120  time    588\n",
      "38   meet    586\n",
      "66    wan    579\n",
      "263  late    568\n",
      "3     wat    536\n",
      "43   come    516\n"
     ]
    }
   ],
   "source": [
    "# Term-Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "sg_corpus_2003 = []\n",
    "\n",
    "# Loop over records and, for each record, append each word of the associated message to the list sg_corpus_2003. This\n",
    "# creates a list of all non-stopword, lemmatized words in all text messages sent by users from Singapore in 2003:\n",
    "# Begin loop:\n",
    "for i in range(len(sg_sms_text_data_2003['Msg_token_nostop_lemmmed'])):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thistext = sg_sms_text_data_2003.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Loop over tokens in the current list:\n",
    "    for word in thistext:\n",
    "        # Append the current token to the master list sg_corpus_2003:\n",
    "        sg_corpus_2003.append(word)\n",
    "\n",
    "# Instantiate a Counter class object from the iterable list sg_corpus_2003:\n",
    "sg_count_of_words_2003 = Counter(sg_corpus_2003)\n",
    "# Convert the Counter object sg_count_of_words_2003 to a Pandas DataFrame:\n",
    "sg_word_counts_2003 = pd.DataFrame.from_dict(sg_count_of_words_2003, orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "sg_word_counts_2003.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "sg_word_counts_2003.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly used words\n",
    "print(sg_word_counts_2003.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike what I saw when analyzing the top 15 \"most common\" words for the full corpus of text messages sent by people from Singapore, there are a fair number single letter/number abbreviations for words in the above list of top 15 \"most common\" words. (In the full corpus, only \"u\" was found.) The appearance of such abbreviations could be due to character limits, which were much more common in the early days of text messaging than they are now, or it could be due to other factors. The English words in the top 15 \"most common\" list all give a sense of meetings and/or get-togethers being planned: \"go,\" \"get,\" \"meet,\" \"time,\" \"come.\" \"Late\" also appears, which could be indicating that the message sender is running late for something. I believe that \"wan\" is short for \"wanna\" and that \"wat\" is short for \"what;\" these terms also indicate some sort of planning occurring in the messages. I do not know if \"lor\" is short for something or if it is a Malay/Tamil word, so I cannot comment on what it indicates about the common topics of text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  count\n",
      "10     go   1435\n",
      "9     get    891\n",
      "195   lor    717\n",
      "238    ok    628\n",
      "214    ur    607\n",
      "116  time    567\n",
      "36   meet    555\n",
      "80    wan    548\n",
      "251  late    546\n",
      "3     wat    517\n",
      "38   come    500\n",
      "53    hey    488\n",
      "89   haha    457\n",
      "153   dun    413\n",
      "74    one    374\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "sing_corpus_2003 = []\n",
    "\n",
    "# Loop over records and, for each record, join the list of non-stopword, lemmatized words into one string, then append\n",
    "# that string to the list of text message strings to be stored in sing_corpus_2003:\n",
    "# Begin loop:\n",
    "for i in range(len(sg_sms_text_data_2003.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thislist = sg_sms_text_data_2003.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Join the tokens in the current list into one string, separating them by commas:\n",
    "    thisstring = ', '.join(thislist)\n",
    "    # Append the current string to the list sing_corpus_2003:\n",
    "    sing_corpus_2003.append(thisstring)\n",
    "\n",
    "# Instantiate a TfidVectorizer object, making sure that inverse document frequency will be calculated:    \n",
    "sing_vectorizer_2003 = TfidfVectorizer(use_idf=True)\n",
    "# Fit the sing_vectorizer_2003 object to the corpus contained in india_corpus:\n",
    "sing_tfidf_vectors_2003 = sing_vectorizer_2003.fit_transform(sing_corpus_2003)\n",
    "\n",
    "# Begin loop over vectors:\n",
    "for i in range(len(sg_sms_text_data_2003.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the tf-idf vector corresponding to the current record, i.e. text message:\n",
    "    tfidf_vector = sing_tfidf_vectors_2003[i]\n",
    "    # Place tf-idf values in a Pandas DataFrame, setting the indices of the DataFrame to the words found in the\n",
    "    # corpus of text messages:\n",
    "    tfidf_results = pd.DataFrame(tfidf_vector.T.todense(), index = sing_vectorizer_2003.get_feature_names(), \\\n",
    "                                 columns = [\"TF-IDF Score\"])\n",
    "    # Keep only those records in tfidf_results which have a non-zero tf-idf score:\n",
    "    nonzero_words = tfidf_results[tfidf_results['TF-IDF Score'] > 0.0]\n",
    "    # If this is the first vector, make the DataFrame of non-zero tf-idf words equal to the current DataFrame \n",
    "    # nonzero_words:\n",
    "    if i == 0:\n",
    "        sing_top_tfidf_scores_2003 = nonzero_words\n",
    "    # Otherwise, append the current DataFrame nonzero_words to the already extant DataFrame sing_top_tfidf_scores_2003: \n",
    "    else:\n",
    "        sing_top_tfidf_scores_2003 = sing_top_tfidf_scores_2003.append(nonzero_words)\n",
    "\n",
    "# Convert the DataFrame sing_top_tfidf_scores_2003 to a list:\n",
    "sing_top_tfidf_words_2003 = sing_top_tfidf_scores_2003.index[sing_top_tfidf_scores_2003['TF-IDF Score'] > 0.0].to_list()\n",
    "\n",
    "# Instantiate a Counter class from the iterable list sing_top_tfidf_words_2003:\n",
    "sing_count_of_top_tfidf_words_2003 = Counter(sing_top_tfidf_words_2003)\n",
    "# Convert the Counter object sing_count_of_top_tfidf_words_2003 to a Pandas DataFrame:\n",
    "sing_top_tfidf_word_counts_2003 = pd.DataFrame.from_dict(sing_count_of_top_tfidf_words_2003, \\\n",
    "                                                         orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "sing_top_tfidf_word_counts_2003.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "sing_top_tfidf_word_counts_2003.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly occuring important words in text messages in 2003:\n",
    "print(sing_top_tfidf_word_counts_2003.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these words were found in the list of top 15 \"most common\" words found in text messages sent by people from Singapore in 2003; those cross-over words largely give a sense that meetings and/or get-togethers are being arranged. \"Lor\" appears in both lists, but I cannot comment on what it indicates about common text message topics. \"Hey\" is a common greeting and does not provide much information about common text message topics; but the appearance of \"haha\" indicates that a message is being softened or that a reaction to a previous message/event is being sent. All in all, these conclusions are the same as the conclusions that I drew for the full corpus of text messages from people from Singapore, but the words themselves seem to be indicating that the primary topic of text conversations is meetings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Text Messages Sent in 2011 </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  count\n",
      "35      haha   6153\n",
      "2          u   4546\n",
      "126       go   2872\n",
      "4306     lol   2807\n",
      "83       get   2127\n",
      "640   hahaha   1746\n",
      "1252    okay   1512\n",
      "219       le   1391\n",
      "141       ok   1155\n",
      "168     late   1139\n",
      "194    think   1127\n",
      "265     yeah   1046\n",
      "123     time   1042\n",
      "16        oh    963\n",
      "789       eh    938\n"
     ]
    }
   ],
   "source": [
    "# Term-Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "sg_corpus_2011 = []\n",
    "\n",
    "# Loop over records and, for each record, append each word of the associated message to the list sg_corpus_2011. This\n",
    "# creates a list of all non-stopword, lemmatized words in all text messages sent by users from Singapore in 2011:\n",
    "# Begin loop:\n",
    "for i in range(len(sg_sms_text_data_2011['Msg_token_nostop_lemmmed'])):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thistext = sg_sms_text_data_2011.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Loop over tokens in the current list:\n",
    "    for word in thistext:\n",
    "        # Append the current token to the master list sg_corpus_2011:\n",
    "        sg_corpus_2011.append(word)\n",
    "\n",
    "# Instantiate a Counter class object from the iterable list sg_corpus_2011:\n",
    "sg_count_of_words_2011 = Counter(sg_corpus_2011)\n",
    "# Convert the Counter object sg_count_of_words_2011 to a Pandas DataFrame:\n",
    "sg_word_counts_2011 = pd.DataFrame.from_dict(sg_count_of_words_2011, orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "sg_word_counts_2011.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "sg_word_counts_2011.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly used words\n",
    "print(sg_word_counts_2011.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list of top 15 \"most common\" words is strikingly different from the list of top 15 \"most common\" words found in text messages from Singaporean people in 2003. Most of these top 15 \"most common\" words now are reactive words/terms like \"haha,\" \"hahaha,\" \"lol,\" \"oh,\" \"ok/okay,\" and \"yeah.\" The appearance of \"go,\" \"late,\" and \"time\" all indicate that arranging get-togethers could still be a common text message conversation topic, but it seems as though the primary topics have shifted to more traditional conversations; perhaps the messages are reacting to something that happened in a class at the National University of Singapore, or reacting to the sharing of a website/YouTube video/image. I note that there seem to be more Malay/Tamil terms (\"le,\" \"eh\") than there were in 2003. This could also indicate a shift from meeting arrangement to more traditional conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  count\n",
      "30      haha   4265\n",
      "119       go   2601\n",
      "4278     lol   2400\n",
      "77       get   1969\n",
      "626   hahaha   1426\n",
      "1234    okay   1424\n",
      "211       le   1345\n",
      "136       ok   1117\n",
      "163     late   1100\n",
      "187    think   1079\n",
      "118     time   1004\n",
      "261     yeah    970\n",
      "770       eh    915\n",
      "20        oh    887\n",
      "197     come    820\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency analysis:\n",
    "\n",
    "# Initialize an empty list:\n",
    "sing_corpus_2011 = []\n",
    "\n",
    "# Loop over records and, for each record, join the list of non-stopword, lemmatized words into one string, then append\n",
    "# that string to the list of text message strings to be stored in sing_corpus_2011:\n",
    "# Begin loop:\n",
    "for i in range(len(sg_sms_text_data_2011.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the list containing non-stopword, lemmatized words of the Message associated with this record:\n",
    "    thislist = sg_sms_text_data_2011.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Join the tokens in the current list into one string, separating them by commas:\n",
    "    thisstring = ', '.join(thislist) \n",
    "    # Append the current string to the list sing_corpus_2011:\n",
    "    sing_corpus_2011.append(thisstring)\n",
    "\n",
    "# Instantiate a TfidVectorizer object, making sure that inverse document frequency will be calculated:    \n",
    "sing_vectorizer_2011 = TfidfVectorizer(use_idf=True)\n",
    "# Fit the sing_vectorizer_2003 object to the corpus contained in india_corpus:\n",
    "sing_tfidf_vectors_2011 = sing_vectorizer_2011.fit_transform(sing_corpus_2011)\n",
    "\n",
    "# Begin loop over vectors:\n",
    "for i in range(len(sg_sms_text_data_2011.Msg_token_nostop_lemmmed)):\n",
    "    # Pluck off the tf-idf vector corresponding to the current record, i.e. text message:\n",
    "    tfidf_vector = sing_tfidf_vectors_2011[i]\n",
    "    # Place tf-idf values in a Pandas DataFrame, setting the indices of the DataFrame to the words found in the\n",
    "    # corpus of text messages:\n",
    "    tfidf_results = pd.DataFrame(tfidf_vector.T.todense(), index = sing_vectorizer_2011.get_feature_names(), \\\n",
    "                                 columns = [\"TF-IDF Score\"])\n",
    "    # Keep only those records in tfidf_results which have a non-zero tf-idf score:\n",
    "    nonzero_words = tfidf_results[tfidf_results['TF-IDF Score'] > 0.0]\n",
    "    # If this is the first vector, make the DataFrame of non-zero tf-idf words equal to the current DataFrame \n",
    "    # nonzero_words:\n",
    "    if i == 0:\n",
    "        sing_top_tfidf_scores_2011 = nonzero_words\n",
    "    # Otherwise, append the current DataFrame nonzero_words to the already extant DataFrame \n",
    "    # sing_top_tfidf_scores_2011:     \n",
    "    else:\n",
    "        sing_top_tfidf_scores_2011 = sing_top_tfidf_scores_2011.append(nonzero_words)\n",
    "\n",
    "# Convert the DataFrame sing_top_tfidf_scores_2011 to a list:\n",
    "sing_top_tfidf_words_2011 = sing_top_tfidf_scores_2011.index[sing_top_tfidf_scores_2011['TF-IDF Score'] > 0.0].to_list()\n",
    "\n",
    "# Instantiate a Counter class from the iterable list sing_top_tfidf_words_2011:\n",
    "sing_count_of_top_tfidf_words_2011 = Counter(sing_top_tfidf_words_2011)\n",
    "# Convert the Counter object sing_count_of_top_tfidf_words_2011 to a Pandas DataFrame:\n",
    "sing_top_tfidf_word_counts_2011 = pd.DataFrame.from_dict(sing_count_of_top_tfidf_words_2011, \\\n",
    "                                                         orient='index').reset_index()\n",
    "# Rename the columns of the DataFrame to be more interpretable by humans:\n",
    "sing_top_tfidf_word_counts_2011.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "# Sort the DataFrame by the count column:\n",
    "sing_top_tfidf_word_counts_2011.sort_values(by=[\"count\"], ascending = False, inplace = True)\n",
    "# Print out the 15 most commonly occuring important words in text messages in 2011:\n",
    "print(sing_top_tfidf_word_counts_2011.nlargest(15,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of top 15 \"most important\" words is almost identical to the list of top 15 \"most common\" words; the only major difference is the appearance of \"u\" and \"hahaha\" in the top 15 most common words and \"come\" and \"get\" in the top 15 most important words. I get the impression from this list of top 15 \"most important\" words that, in text messages sent by people from Singapore in 2011, two of the most common conversation topics were setting up meetings/get-togethers and reacting to something that the sender either received, saw, or heard. The latter topic seems to indicate a shift in text message usage from primarily setting up meetings to informal conversations as well as setting up meetings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section8\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Word Embedding Analysis Based on TF-IDF Analysis </h3> <br>\n",
    "Looking at the most common and \"most important\" words found in text messages can provide some indication of what their most common topics are, but it would be nice to augment such findings with other, complimentary analyses that can shed additional light onto text message topics. In the next two sections, I extend the results of <a href='#Section6'> Important Words in Messages: TF and TF-IDF Analysis </a> by looking at word embeddings and n-grams. This particular section focuses on word embeddings. The idea behind word embeddings is that words which co-occur in the same contexts tend to have similar meanings (Source: Codecademy.) That is to say, words which are used in similar ways tend to \"cluster together,\" in a sense. Indeed, if one were to \"decompose\" words into numerical properties--that is to say, if one assigns numeric values to vector representations of words--word-vectors associated with words used in similar contexts should lie near each other in the n-dimensional word-vector space. If one then looks at the distances between word-vectors, one would expect that words which are used in similar contexts would have small distances between them. By using the Gensim model Word2Vec, I will be able to develop word embeddings models specific to the corpuses of text messages from people from Singapore, from India, and from the United States, and from there determine which word-vectors \"cluster\" around the word-vectors associated with words found in the \"most important\" words list of each country. By seeing what words people use in similar contexts to those particular words, I can get a better sense of what text message senders tend to want to talk about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from Singapore: </h5><br>\n",
    "First, word embeddings must be determined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4848644, 7101180)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the DataFrame column Msg_token_nostop_lemmed, which contains lists of the lemmatized, tokenized text of every\n",
    "# Message sent by users from Singapore, to a list: \n",
    "sg_corpus = sg_sms_text_data.Msg_token_nostop_lemmmed.to_list()\n",
    "\n",
    "# Use Gensim's Word2Vec() class to build a word embeddings model based on the corpus of texts from users from \n",
    "# Singapore. I will be using a skipgrams model with a maximum distance between words of 5; the model will ignore \n",
    "# words with a frequency of less than 15. Hierarchical softmax will be used for model training.\n",
    "sg_w2v_model = gensim.models.Word2Vec(window=5, sg = 1, hs = 1, min_count=15, workers=2)\n",
    "# Build the model vocabulary:\n",
    "sg_w2v_model.build_vocab(sg_corpus, progress_per=10000)\n",
    "# Train the model on the corpus of Messages from users from Singapore. Since the model's vocabulary has already been\n",
    "# built, use it to determine the total number of \"sentences\" in the corpus:\n",
    "sg_w2v_model.train(sg_corpus, total_examples=sg_w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that word embeddings have been determined, I will see which word-vectors have the smallest distances from word-vectors associated with particular \"most important\" words. First, I pick a word found in the \"most important\" words lists of text messages from people from Singapore, India, and the United States to compare word embeddings from the three countries. That word is \"come.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 0.677050769329071),\n",
       " ('back', 0.5553478002548218),\n",
       " ('call', 0.5058311820030212),\n",
       " ('meet', 0.4714341461658478),\n",
       " ('fetch', 0.45734483003616333),\n",
       " ('reach', 0.4547424912452698),\n",
       " ('home', 0.4410226047039032),\n",
       " ('see', 0.4241775572299957),\n",
       " ('daddy', 0.4218423068523407),\n",
       " ('leave', 0.41735950112342834)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with come:\n",
    "sg_w2v_model.wv.most_similar(\"come\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the words which Singaporeans used in similar contexts to \"come\" seem to indicate talking about meetings or meeting up with people: \"go,\" \"late,\" \"meet;\" the appearance of \"fetch\" may also indicate talking about a meet-up, say if a person would like to retrieve a forgotten item or get food before a meet-up. I am quite puzzled by the appearance of \"interchange\" and \"daddy;\" perhaps this is due to my lack of understanding about slang use in Singapore. \"Call\" and \"reach\" also appear on the list, perhaps because text message senders try to arrange phone calls by text message in addition to arranging in-person meet-ups. <br> \n",
    "<br> I now investigate which words are most similar to some other words in the \"most important\" words list of text messages sent by from people Singapore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hahaha', 0.8336943984031677),\n",
       " ('yeah', 0.7634389996528625),\n",
       " ('ooo', 0.7543618679046631),\n",
       " ('p', 0.7193090915679932),\n",
       " ('okay', 0.6843717694282532),\n",
       " ('ohh', 0.6740002036094666),\n",
       " ('omg', 0.6638556718826294),\n",
       " ('lol', 0.6188974976539612),\n",
       " ('yeap', 0.6003279685974121),\n",
       " ('icic', 0.5861074328422546)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with \"haha:\"\n",
    "sg_w2v_model.wv.most_similar(\"haha\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these words/terms indicate that the sender is reacting to something, be it a previous text message, an event, a joke, something that happened in class or on the street, a URL, or an image. I am puzzled by the appearance of the letter \"p,\" however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('haha', 0.5469188094139099),\n",
       " ('yeah', 0.5253894329071045),\n",
       " ('know', 0.511877715587616),\n",
       " ('hahaha', 0.5064802765846252),\n",
       " ('hmm', 0.48492568731307983),\n",
       " ('okay', 0.46863994002342224),\n",
       " ('maybe', 0.46269094944000244),\n",
       " ('like', 0.4473291337490082),\n",
       " ('p', 0.4449206590652466),\n",
       " ('leh', 0.4343850612640381)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with \"think:\"\n",
    "sg_w2v_model.wv.most_similar(\"think\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of words which \"cluster\" near think are a bit harder to parse through than the list for \"come\" and \"haha.\" I could easily imagine the following phrases being found in text messages based on this list: \"I think I know ...\", \"I think I have the idea ...\", \"I think I want ...\", \"Yeah, I think ...\" Clearly, there are many contexts in which \"think\" is used. Looking at the most common two-, three-, and four-word groupings could shed some additional light on this word list. I explore n-grams found in the corpus of Singaporean texts in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 0.5656272172927856),\n",
       " ('free', 0.5498029589653015),\n",
       " ('orchard', 0.52894526720047),\n",
       " ('2pm', 0.5155308842658997),\n",
       " ('mrt', 0.5097926259040833),\n",
       " ('mit', 0.47846361994743347),\n",
       " ('marina', 0.4770604372024536),\n",
       " ('come', 0.4714341163635254),\n",
       " ('confirm', 0.4714074432849884),\n",
       " ('pm', 0.45856189727783203)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with \"meet:\"\n",
    "sg_w2v_model.wv.most_similar(\"meet\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words (and times) clearly indicate that the specifics of meetings are being discussed and/or that meeting times and locations are being confirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from India: </h5><br>\n",
    "First, word embeddings must be determined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528179, 1328310)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the DataFrame column Msg_token_nostop_lemmed, which contains lists of the lemmatized, tokenized text of every\n",
    "# Message sent by users from India, to a list: \n",
    "in_corpus = in_sms_text_data.Msg_token_nostop_lemmmed.to_list()\n",
    "\n",
    "# Build a skipgram-based Word2Vec embedding model using hierarchical softmax for training. Ignore words with frequencies\n",
    "# less than 20; take a maximum distance between words of 5:\n",
    "in_w2v_model = gensim.models.Word2Vec(window=5, sg = 1, hs = 1, min_count=20, workers=2)\n",
    "# Build the model vocabulary:\n",
    "in_w2v_model.build_vocab(in_corpus, progress_per=10000)\n",
    "# Train the model on the corpus of Messages from users from India:\n",
    "in_w2v_model.train(in_corpus, total_examples=in_w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now see which word-vectors have the smallest distances from the word-vector associated with \"come,\" and compare it to the words which were most similar to \"come\" in the Singaporean corpus of text messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ill', 0.4594912528991699),\n",
       " ('go', 0.45673564076423645),\n",
       " ('way', 0.43768444657325745),\n",
       " ('yesterday', 0.41943010687828064),\n",
       " ('reach', 0.41530388593673706),\n",
       " ('meet', 0.4042049050331116),\n",
       " ('well', 0.3788103759288788),\n",
       " ('tonight', 0.37806010246276855),\n",
       " ('leave', 0.3718056380748749),\n",
       " ('near', 0.3620550036430359)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with come:\n",
    "in_w2v_model.wv.most_similar(\"come\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to parse what this list of words associated with \"come\" indicates in terms of context. The appearance of \"meet,\" \"go,\" \"get,\" \"tomorrow,\" and \"leave\" seem to indicate either meeting planning or an update to meeting arrangements; the words associated with \"come\" in the Singaporean corpus also indicated meetings being arranged, so in that sense at least some of the common text message topics seem to be similar. I am unsure what to think of the appearance of \"ill,\" \"reach,\" \"well,\" and \"yesterday.\" What contexts could these words be indicating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sorry', 0.41783469915390015),\n",
       " ('wait', 0.38689756393432617),\n",
       " ('tell', 0.38523921370506287),\n",
       " ('pick', 0.3791552484035492),\n",
       " ('sir', 0.36521121859550476),\n",
       " ('reach', 0.3616574704647064),\n",
       " ('mobile', 0.3523366451263428),\n",
       " ('please', 0.3484105169773102),\n",
       " ('cal', 0.3422373831272125),\n",
       " ('miss', 0.328093945980072)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate words that are found in similar contexts to common/important words in Messages from users from India.\n",
    "# First, see what words' vectorizations have the smallest cosine distances from the word-vector associated with \"call:\"\n",
    "in_w2v_model.wv.most_similar(\"call\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list of words above, I get the sense that there are two common topics of conversation around phone calls: one apologizes for missing a phone call (\"sorry,\" \"miss\") and the other arranges a time for a phone call (\"please,\" \"reach,\" \"min\"). I am not sure what to make of the appearance of \"wait,\" \"pick,\" and \"cal.\" Is \"cal\" simply \"call\" misspelled? Do text message senders often request that someone wait before calling them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hear', 0.42281898856163025),\n",
       " ('talk', 0.3833679258823395),\n",
       " ('birthday', 0.36532172560691833),\n",
       " ('food', 0.3635594844818115),\n",
       " ('late', 0.3553018569946289),\n",
       " ('miss', 0.3467169404029846),\n",
       " ('yesterday', 0.342174768447876),\n",
       " ('pa', 0.3417445421218872),\n",
       " ('catch', 0.3388882279396057),\n",
       " ('worry', 0.3380366265773773)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with \"dear:\"\n",
    "in_w2v_model.wv.most_similar(\"dear\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I frankly have no idea how to make sense of this list of words. The word \"qatar\" in particular puzzles me. If anything, I get the sense that \"dear\" tends to be associated with talking to a loved one, perhaps saying that it is good to hear from the loved one, or not to worry about them, or that they will speak to the loved one soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('way2sms', 0.4166349470615387),\n",
       " ('via', 0.4090697765350342),\n",
       " ('com', 0.3968253433704376),\n",
       " ('contact', 0.3760831356048584),\n",
       " ('mobile', 0.3689061403274536),\n",
       " ('thats', 0.36600548028945923),\n",
       " ('ur', 0.3458503186702728),\n",
       " ('get', 0.33690452575683594),\n",
       " ('must', 0.32537001371383667),\n",
       " ('account', 0.31395503878593445)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with \"send:\"\n",
    "in_w2v_model.wv.most_similar(\"send\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words intrigued me. In it appear words that I would naturally associate with sending a message to an email account (\"mail,\" \"contact\"), but also words which indicate that an SMS-sending service is responsible for sending the message (\"way2sms,\" \"via,\" \"com,\" \"mobile\" ). Indeed, this list of words prompted me to investigate n-grams for these particular corpuses of text messages: I wanted to test my hypothesis that an SMS-message sending service was being used by Indian people living in Singapore by looking at common phrases in the text messages: SMS-message sending services usually tag a message with a phrase which indicates the name of the service and how it was sent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from the United States: </h5><br>\n",
    "First, word embeddings must be determined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353368, 932940)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the DataFrame column Msg_token_nostop_lemmed, which contains lists of the lemmatized, tokenized text of every\n",
    "# Message sent by users from the United States, to a list: \n",
    "us_corpus = us_sms_text_data.Msg_token_nostop_lemmmed.to_list()\n",
    "\n",
    "# Build a skipgram-based Word2Vec embedding model using hierarchical softmax for training. Ignore words with frequencies\n",
    "# less than 20; take a maximum distance between words of 5:\n",
    "us_w2v_model = gensim.models.Word2Vec(window=5, sg = 1, hs = 1, min_count=20, workers=2)\n",
    "# Build the model vocabulary:\n",
    "us_w2v_model.build_vocab(us_corpus, progress_per=10000)\n",
    "# Train the model on the corpus of Messages from users from the United States:\n",
    "us_w2v_model.train(us_corpus, total_examples=us_w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now see which word-vectors have the smallest distances from the word-vector associated with \"come,\" and compare it to the words which were most similar to \"come\" in the Singaporean and Indian corpuses of text messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pick', 0.47239160537719727),\n",
       " ('guy', 0.4385787844657898),\n",
       " ('friday', 0.4266737103462219),\n",
       " ('go', 0.42589280009269714),\n",
       " ('try', 0.4181239902973175),\n",
       " ('bring', 0.4177267253398895),\n",
       " ('k', 0.41761502623558044),\n",
       " ('home', 0.4132457673549652),\n",
       " ('plan', 0.3985275328159332),\n",
       " ('tonight', 0.38866499066352844)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with come:\n",
    "us_w2v_model.wv.most_similar(\"come\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these words indicate to me that the sender is requesting that something to be brought to a planned meet-up (\"bring,\" \"pick\"); others indicate meeting planning (\"plan,\" \"tomorrow,\" \"Friday,\" \"meet,\" \"happen\"). This is largely the same sense of words associated with \"come\" in the Singaporean corpus, although that corpus seemed to emphasize the meeting planning over requests for things. The corpus of text messages sent by people from India also indicated that meeting planning was a common topic. I am puzzled by the appearance of \"guy\" in the above word list, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ya', 0.44625231623649597),\n",
       " ('enjoy', 0.4452831745147705),\n",
       " ('glad', 0.433908075094223),\n",
       " ('soon', 0.4263707101345062),\n",
       " ('miss', 0.4072240889072418),\n",
       " ('oh', 0.4071563482284546),\n",
       " ('much', 0.40257102251052856),\n",
       " ('home', 0.38858869671821594),\n",
       " ('really', 0.3758065402507782),\n",
       " ('baby', 0.35851016640663147)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate words that are found in similar contexts to common/important words in Messages from users from the \n",
    "# United States. First, see what words' vectorizations have the smallest cosine distances from the word-vector \n",
    "# associated with \"love:\"\n",
    "us_w2v_model.wv.most_similar(\"love\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these words clearly indicate that a common topic of conversation is messages of endearment: \"ya,\" \"miss,\" \"much.\" Others may indicate a text message sender's strong preference for something (\"enjoy,\" \"good\"). But overall, I get the sense that American text message senders who live in Singapore are telling loved ones how much they are loved and missed, and perhaps that they are looking forward to a visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('away', 0.4672279357910156),\n",
       " ('girl', 0.3876877427101135),\n",
       " ('p', 0.37938761711120605),\n",
       " ('every', 0.3729708194732666),\n",
       " ('ugh', 0.36576157808303833),\n",
       " ('little', 0.3645671308040619),\n",
       " ('awesome', 0.35590261220932007),\n",
       " ('least', 0.3514868915081024),\n",
       " ('5', 0.3483671247959137),\n",
       " ('want', 0.34076106548309326)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with \"like:\"\n",
    "us_w2v_model.wv.most_similar(\"like\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to make sense of this list. If anything, I get the sense of \"like\" being used in typical American slang, which would make sense given that these text message senders are from the United States and that \"like\" has been an important part of American slang for several decades. I am puzzled by the appearance of \"p,\" as I genuinely do not know what it could mean in terms of slang or what it could be an abbreviation for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi', 0.5895227193832397),\n",
       " ('ricky', 0.5678653120994568),\n",
       " ('may', 0.5674651861190796),\n",
       " ('email', 0.48925429582595825),\n",
       " ('problem', 0.48653507232666016),\n",
       " ('please', 0.48013773560523987),\n",
       " ('min', 0.4721827507019043),\n",
       " ('g', 0.4600663483142853),\n",
       " ('tomorrow', 0.459259033203125),\n",
       " ('thank', 0.45678117871284485)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what words' vectorizations have the smallest cosine distances from the word-vector associated with \"thanks:\"\n",
    "us_w2v_model.wv.most_similar(\"thanks\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few of these words indicate that the text message is thanking someone for doing something or sending something (\"email,\" \"thank,\" \"please\"). \"Hi\" is a common greeting which may open text messages which thank someone for doing something/sending something. I am puzzled by the appearances of \"g\" and \"ricky.\" Perhaps the text message sender is thanking someone named Ricky for doing something for them or sending them something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section9\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> The Most Common Two-, Three-, and Four-Word Groups of Words </h3> <br>\n",
    "TF and TF-IDF analyses, particularly when combined with word embeddings, give a sense of some of the general topics of text messages sent by people from Singapore, India, and the United States. However, looking at how frequently particular groups of words appear can complement my previous analysis and perhaps provide more insight. Knowing that \"meet,\" \"go\", and \"come\" are common and/or important words in text messages gives a sense of meetings being planned, but seeing groups of words like \"meet you\" or \"come for dinner\" would give an even clearer idea of text message topics. I was prodded down this path by my results from my investigation into word embeddings of words found in text messages sent by people from India: words which have similar contexts to \"send\" in that corpus included \"way2sms,\" \"via,\" \"com,\" and \"mobile.\" Could there be a lot of text messages sent by people from India which were sent by a \"way2sms\" messaging service? Looking for phrases like \"sent via way2sms\" could answer that question. <br>\n",
    "I decided to look through the corpuses of texts from people from Singapore, India, and the United States to find the most common two-, three-, and four-word phrases which appeared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from Singapore: </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore Corpus Bigrams:\n",
      "[('wan na', 567), ('haha okay', 354), ('u go', 328), ('u wan', 322), ('haha yeah', 271), ('u get', 236), ('wat time', 230), ('r u', 202), ('go home', 197), ('lol haha', 191), ('reach home', 180), ('haha haha', 177), ('love u', 174), ('meet u', 168), ('go back', 167), ('come back', 165), ('see u', 161), ('u want', 152), ('time u', 150), ('tell u', 147), ('u come', 144), ('haha u', 144), ('gon na', 144), ('call u', 142), ('next time', 139)]\n",
      "Singapore Corpus Trigrams:\n",
      "[('wan na go', 116), ('u wan na', 96), ('wat time u', 74), ('reach home le', 61), ('sob sob sob', 58), ('wan na meet', 55), ('happy new year', 35), ('u wan 2', 33), ('wan na play', 28), ('sorry late reply', 28), ('wan na eat', 28), ('haha okay okay', 26), ('haha ya lo', 26), ('u wan go', 25), ('u reach home', 25), ('let u know', 23), ('x x haha', 23), ('go slp le', 22), ('wan na come', 21), ('love u dear', 21), ('love u much', 21), ('haha haha okay', 21), ('haha dun know', 21), ('wat r u', 20), ('u come back', 19)]\n",
      "Singapore Corpus Four-grams:\n",
      "[('u wan na meet', 14), ('dear reach home le', 14), ('love u love u', 13), ('wat time u wan', 12), ('u wan na go', 12), ('u love u love', 11), ('wan na play mahjong', 11), ('3 mechanical engineering 4', 10), ('2 alvin chia 3', 10), ('ya ya ya ya', 10), ('alvin chia 3 mechanical', 9), ('chia 3 mechanical engineering', 9), ('wan na join u', 8), ('lol haha ya lo', 8), ('wat time u go', 7), ('dun disturb u liao', 7), ('u wan 2 go', 7), ('boo boo boo boo', 7), ('happy chinese new year', 7), ('xiao long bao buffet', 7), ('lo sob sob sob', 7), ('u wan 2 come', 6), ('niao niao niao niao', 6), ('u wan na play', 6), ('u reach home le', 6)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty string:\n",
    "sg_corpus = ''\n",
    "\n",
    "# Loop over the column of lists of lemmatized words in the DataFrame and add each word to the string sg_corpus:\n",
    "# Begin loop:\n",
    "for i in range(len(sg_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Select the current list of lemmatized words:\n",
    "    thislist = sg_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Begin loop over list of lemmatized words:\n",
    "    for j in range(len(thislist)):\n",
    "        # Select the current word and add a space after it:\n",
    "        thisword = thislist[j] + \" \"\n",
    "        # Append the word+space to the string sg_corpus:\n",
    "        sg_corpus += thisword\n",
    "\n",
    "# The string sg_corpus contains every lemmatized word in every text message from users from Singapore. Use the NLTK \n",
    "# method ngrams to look for two-word phrases in sg_corpus. ngrams() only accepts a list of tokenized words, so tokenize\n",
    "# sg_corpus in the method call to ngrams(): \n",
    "sg_corpus_bigrams = ngrams(word_tokenize(sg_corpus), 2)\n",
    "# ngrams() returns an iterator, so generate a list of the two-word phrases found using it; this is done \n",
    "# via list comprehension. I chose to use list comprehension rather than to call list() on sg_corpus_bigrams because\n",
    "# the list comprehension method makes the output easier to read, in my opinion: \n",
    "sg_corpus_bigrams = [' '.join(bigrams) for bigrams in sg_corpus_bigrams]\n",
    "# Instantiate a Counter object using the list of two-word phrases sg_corpus_bigrams:\n",
    "sg_corpus_bigrams_frequency = Counter(sg_corpus_bigrams)\n",
    "\n",
    "# Print out the 25 most common two-word phrases (i.e., bigrams) using the .most_common() method:\n",
    "print(\"Singapore Corpus Bigrams:\")\n",
    "print(sg_corpus_bigrams_frequency.most_common(25))\n",
    "\n",
    "# Use ngrams() to look for three-word phrases in the string sg_corpus. Once again, sg_corpus must be tokenized, \n",
    "# and I again do this in the method call to ngrams():\n",
    "sg_corpus_trigrams = ngrams(word_tokenize(sg_corpus), 3)\n",
    "# Use list comprehension to generate a list from the iterator sg_corpus_trigrams:\n",
    "sg_corpus_trigrams = [' '.join(trigrams) for trigrams in sg_corpus_trigrams]\n",
    "# Instantiate a Counter object using the list of three-word phrases sg_corpus_trigrams:\n",
    "sg_corpus_trigrams_frequency = Counter(sg_corpus_trigrams)\n",
    "\n",
    "# Print out the 25 most common three-word phrases (i.e., trigrams) using the .most_common() method:\n",
    "print(\"Singapore Corpus Trigrams:\")\n",
    "print(sg_corpus_trigrams_frequency.most_common(25))\n",
    "\n",
    "# Use ngrams() to look for four-word phrases in the string sg_corpus, tokenizing the string in the method call:\n",
    "sg_corpus_quadgrams = ngrams(word_tokenize(sg_corpus), 4)\n",
    "# Use list comprehension to generate a list from the iterator sg_corpus_quadgrams:\n",
    "sg_corpus_quadgrams = [' '.join(quadgrams) for quadgrams in sg_corpus_quadgrams]\n",
    "# Instantiate a Counter object using the list of four-word phrases sg_corpus_quadgrams:\n",
    "sg_corpus_quadgrams_frequency = Counter(sg_corpus_quadgrams)\n",
    "\n",
    "# Print out the 25 most common four-word phrases (i.e., four-grams) using the .most_common() method:\n",
    "print(\"Singapore Corpus Four-grams:\")\n",
    "print(sg_corpus_quadgrams_frequency.most_common(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can immediately identify three common text message topics based on these phrases. One topic, as previously identified, is arranging meetups and/or meetings; this is evidenced by the phrases \"u wan na meet, \"\"u wan 2 go,\" \"u wan 2 come,\" \"u wan na play,\" \"wan na come,\" \"wat time,\" \"wan na go,\" \"meet u,\" \"wan na join u\" and \"wan na eat.\" Another topic is interactions with loved ones, which includes telling them that they are loved and referencing past or future communications; all of this is evidenced by the phrases \"love u love u,\" \"u reach home,\" \"dear reach home le,\" \"call u,\" \"sorry late reply,\" \"love u much,\" and \"love u dear.\"  The third easily identifiable text message topic is reactions to something: \"lol haha ya lo,\" \"lo sob sob sob,\" \"boo boo boo boo,\" \"haha u,\" \"haha yeah,\" \"haha haha.\" A few phrases seem to be talking about particular classes and/or classrooms at the National University of Singapore: \"3 mechanical engineering 4,\" \"alvin chia 3 mechanical,\" \"chia 3 mechanical engineering.\" The last thing I noticed is that text messages communicating good wishes for the new year (\"happy new year,\" \"happy chinese new year\") were not uncommon in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-grams analysis conducted above was very insightful; however, I wanted to try a different method of identifying n-grams, and bigrams (i.e., two-word phrases) in particular, that I came across while reading an article online. That method involves using the Phrases class of Gensim's models package. I expect that the results obtained from implementing Phrases will be largely in agreement with those obtained from implementing NLTK's ngrams with n (i.e., the degree of the n-gram) set to 2; however, learning a new method and comparing its performance to that of a method I am already familiar with is very worthwhile. I should note that the bigrams found by Phrases are very sensitive to the value of min_count, which is the low threshold for word counts and bigram counts to be included; or, to borrow the specific phrasing of Phrase's <a href = \"https://radimrehurek.com/gensim/models/phrases.html\"> documentation</a>, \"min_count  ... ignores all words and bigrams with total collected count lower than this value.\" Thus, the list of top 15 bigrams found by ngrams and Phrases may not match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        bigram\n",
      "0        u_wan\n",
      "1       wan_na\n",
      "2      go_back\n",
      "3      go_home\n",
      "4    haha_okay\n",
      "5     wat_time\n",
      "6    come_back\n",
      "7          r_u\n",
      "8   reach_home\n",
      "9    next_time\n",
      "10      gon_na\n",
      "11   next_week\n",
      "12    let_know\n",
      "13     sob_sob\n",
      "14      love_u\n",
      "15   haha_yeah\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame column Msg_token_nostop_lemmed, which contains lists of the lemmatized, tokenized text of every\n",
    "# Message sent by users from Singapore, to a list:\n",
    "sg_corpus = sg_sms_text_data.Msg_token_nostop_lemmmed.to_list()\n",
    "\n",
    "# Use the Gensim method Phrases() to detect two-word phrases (i.e., bigrams) in the list of lists of lemmatized words \n",
    "# sg_corpus. \n",
    "# While I used ngrams() above to accomplish this task, I wanted to get practice using other ways of finding multi-word\n",
    "# expressions in strings using alternative methods.\n",
    "bigram_transformer = Phrases(sg_corpus, min_count=100, threshold=2)\n",
    "\n",
    "# Save the minimal state information needed to use the bigram_transformer object as a .freeze() object; this allows me \n",
    "# faster access to the information in the Phrases object constructed above:\n",
    "sg_bigrams = bigram_transformer.freeze()\n",
    "\n",
    "# Initialize an empty list:\n",
    "bigram_list = []  \n",
    "\n",
    "# Loop over keys in the sg_bigrams.phasegrams dictionary to retrieve the actual text of the bigrams found:\n",
    "for bigram in sg_bigrams.phrasegrams.keys():\n",
    "    bigram_list.append(bigram)\n",
    "\n",
    "# Convert the list of bigrams found above to a Pandas DataFrame:\n",
    "bigram_collection = pd.DataFrame(bigram_list, columns=['bigram'])\n",
    "\n",
    "# Print out the DataFrame of bigrams found:\n",
    "print(bigram_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases identified 16 two-word phrases with the min_count and threshold parameters I used. Comparing these 16 bigrams to those found by ngrams, I see that 13 of them—'u wan', 'wan na', 'go back', 'go home',  'haha okay', 'wat time', 'come back', 'r u', 'reach home', 'next time', 'gon na', 'love u', and 'haha yeah'—match. The bigrams 'next week', 'let know', and 'sob sob' were found by Phrases but not by ngrams; the bigrams 'u go', 'u get', 'lol haha', 'haha haha', 'meet u', 'see u', 'u want', 'time u', 'tell u', 'u come', 'haha u', and 'call u' were found by ngrams and not Phrases. It appears as though ngrams is more likely to find two-word phrases containing the word \"u\" than Phrases() is; I am not sure why this is. Perhaps it is because of the min_count and threshold values I used in my function call to Phrases. However, it is apparent that about half the bigrams found by ngrams were also found by Phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from India: </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Corpus Bigrams:\n",
      "[('`` ``', 102), ('k k', 93), ('u r', 84), ('r u', 76), ('take care', 51), ('good morning', 41), ('via way2sms', 37), ('way2sms com', 37), ('send via', 31), ('let know', 31), ('good night', 30), ('ok sure', 30), ('call late', 26), ('happy birthday', 25), ('oh ok', 24), ('come home', 23), ('hi hi', 23), ('u knw', 22), ('call u', 21), ('u u', 20), ('love u', 20), ('u get', 20), ('oh k', 19), ('gud ni8', 19), ('go 2', 19)]\n",
      "India Corpus Trigrams:\n",
      "[('`` `` ``', 73), ('via way2sms com', 37), ('send via way2sms', 29), ('begin vcard version', 18), ('vcard version decimal', 18), ('version decimal n', 18), ('x class private', 17), ('class private end', 17), ('private end vcard', 17), ('oh k k', 11), ('k k k', 11), ('hi hi hi', 11), ('tel cell x', 10), ('cell x class', 10), ('hw r u', 9), ('wat abt u', 9), ('wer r u', 9), ('tc send via', 8), ('abhi send via', 8), ('happy new year', 8), ('sorry call late', 8), ('search apt opportunity', 7), ('\\\\ `` ``', 7), ('`` \\\\ ``', 7), ('u didnt lyk', 6)]\n",
      "India Corpus Four-grams:\n",
      "[('`` `` `` ``', 64), ('send via way2sms com', 29), ('begin vcard version decimal', 18), ('vcard version decimal n', 18), ('x class private end', 17), ('class private end vcard', 17), ('tel cell x class', 10), ('cell x class private', 10), ('tc send via way2sms', 8), ('abhi send via way2sms', 8), ('sorry call late meet', 6), ('bday x anniversary photo', 6), ('x anniversary photo type', 6), ('anniversary photo type jpeg', 6), ('photo type jpeg encoding', 6), ('type jpeg encoding base', 6), ('jpeg encoding base x', 6), ('encoding base x class', 6), ('base x class private', 6), ('`` `` \\\\ ``', 6), ('`` \\\\ `` ``', 6), ('hi hi hi hi', 6), ('private end vcard begin', 5), ('end vcard begin vcard', 5), ('vcard begin vcard version', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty string:\n",
    "in_corpus = ''\n",
    "\n",
    "# Loop over the column of lists of lemmatized words in the DataFrame and add each word to the string in_corpus:\n",
    "# Begin loop:\n",
    "for i in range(len(in_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Select the current list of lemmatized words:\n",
    "    thislist = in_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Begin loop over list of lemmatized words:\n",
    "    for j in range(len(thislist)):\n",
    "        # Select the current word and add a space after it:\n",
    "        thisword = thislist[j] + \" \"\n",
    "        # Append the word+space to the string in_corpus:\n",
    "        in_corpus += thisword\n",
    "\n",
    "# Use ngrams() to look for two-word phrases in the string in_corpus, tokenizing the string in the method call:\n",
    "in_corpus_bigrams = ngrams(word_tokenize(in_corpus), 2)\n",
    "# Use list comprehension to generate a list from the iterator in_corpus_bigrams:\n",
    "in_corpus_bigrams = [' '.join(bigrams) for bigrams in in_corpus_bigrams]\n",
    "# Instantiate a Counter object using the list of two-word phrases in_corpus_bigrams:\n",
    "in_corpus_bigrams_frequency = Counter(in_corpus_bigrams)\n",
    "\n",
    "# Print out the 25 most common two-word phrases using the .most_common() method:\n",
    "print(\"India Corpus Bigrams:\")\n",
    "print(in_corpus_bigrams_frequency.most_common(25))\n",
    "\n",
    "# Use ngrams() to look for three-word phrases in the string in_corpus, tokenizing the string in the method call:\n",
    "in_corpus_trigrams = ngrams(word_tokenize(in_corpus), 3)\n",
    "# Use list comprehension to generate a list from the iterator in_corpus_trigrams:\n",
    "in_corpus_trigrams = [' '.join(trigrams) for trigrams in in_corpus_trigrams]\n",
    "# Instantiate a Counter object using the list of three-word phrases in_corpus_trigrams:\n",
    "in_corpus_trigrams_frequency = Counter(in_corpus_trigrams)\n",
    "\n",
    "# Print out the 25 most common three-word phrases using the .most_common() method:\n",
    "print(\"India Corpus Trigrams:\")\n",
    "print(in_corpus_trigrams_frequency.most_common(25))\n",
    "\n",
    "# Use ngrams() to look for four-word phrases in the string in_corpus, tokenizing the string in the method call:\n",
    "in_corpus_quadgrams = ngrams(word_tokenize(in_corpus), 4)\n",
    "# Use list comprehension to generate a list from the iterator in_corpus_quadgrams:\n",
    "in_corpus_quadgrams = [' '.join(quadgrams) for quadgrams in in_corpus_quadgrams]\n",
    "# Instantiate a Counter object using the list of four-word phrases in_corpus_quadgrams:\n",
    "in_corpus_quadgrams_frequency = Counter(in_corpus_quadgrams)\n",
    "\n",
    "# Print out the 25 most common four-word phrases using the .most_common() method:\n",
    "print(\"India Corpus Four-grams:\")\n",
    "print(in_corpus_quadgrams_frequency.most_common(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My suspicions appear to be confirmed that some people from India who live in Singapore used an SMS-sending service to send their messages; this is confirmed explicitly by the appearance of the phrase \"send via way2sims com.\" I suspect that the phrases \"begin vcard version decimal,\" \"vcard decimal version n,\" \"x class private end,\" \"class private end vcard\", \"encoding base x class\", \"base x class private\", \"private end vcard begin,\" \"end vcard begin vcard,\" and \"vcard begin vcard version\" are also related to the \"way2sms\" SMS-sending service. It appears as though at least one individual was looking for an apartment (\"search apt opportunity\"), and combinations of empty strings appear quite often for reasons I do not understand. Looking at actual text message topics, there appear to be a class of conversations with loved ones (\"love u,\" \"gud ni8,\" \"hw r u,\" \"sorry call late,\" \"good night,\" \"good morning\", \"happy birthday\") and some reactionary messages (\"oh ok,\" \"ok sure,\" \"oh k,\" \"k k,\" \"oh k k\"). Other phrases are harder to parse given the frequency with which letters and/or numbers are substituted for words, but they seem to be asking where someone is (\"r u\", \"wer r u\") or asking their opinion (\"wat abt u\", \"u knw\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Texts from people from the United States: </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States Corpus Bigrams:\n",
      "[('gon na', 99), ('let know', 75), ('hi ricky', 57), ('wan na', 46), ('thanks hi', 45), ('u get', 25), ('get back', 19), ('ok thanks', 19), ('last night', 18), ('u know', 18), ('thanks ok', 17), ('caw caw', 17), ('take care', 16), ('make sure', 15), ('come home', 15), ('want come', 15), ('r u', 15), ('new year', 15), ('great day', 15), ('love much', 15), ('hi sue', 14), ('lol u', 13), ('next week', 13), ('please start', 13), ('go back', 12)]\n",
      "United States Corpus Trigrams:\n",
      "[('thanks hi ricky', 17), ('happy new year', 11), ('gon na get', 10), ('let u know', 10), ('gon na go', 9), ('hi ing cheong', 8), ('hi yeng chin', 8), ('please let know', 7), ('email address email', 7), ('think gon na', 6), ('choa chu kang', 6), ('please start first', 6), ('thanks ok thanks', 5), ('let know get', 5), ('chu kang mrt', 5), ('time per night', 4), ('take good care', 4), ('wish great day', 4), ('probably gon na', 4), ('make feel well', 4), ('say gon na', 4), ('thanks hi stewart', 4), ('villa verde unit', 4), ('verde unit tell', 4), ('street approx build', 4)]\n",
      "United States Corpus Four=grams:\n",
      "[('choa chu kang mrt', 5), ('villa verde unit tell', 4), ('email address email thank', 4), ('address email thank hi', 4), ('gentleman treat dignity respect', 3), ('number street approx build', 3), ('hi interest villa verde', 3), ('interest villa verde unit', 3), ('street approx build gsf', 3), ('approx build gsf area', 3), ('build gsf area thanks', 3), ('gsf area thanks min', 3), ('mr kan 4 verde', 3), ('outside choa chu kang', 3), ('hi ricky need check', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty string:\n",
    "us_corpus = ''\n",
    "\n",
    "# Loop over the column of lists of lemmatized words in the DataFrame and add each word to the string us_corpus:\n",
    "# Begin loop:\n",
    "for i in range(len(us_sms_text_data.Msg_token_nostop_lemmmed)):\n",
    "    # Select the current list of lemmatized words:\n",
    "    thislist = us_sms_text_data.Msg_token_nostop_lemmmed.iloc[i]\n",
    "    # Begin loop over list of lemmatized words:\n",
    "    for j in range(len(thislist)):\n",
    "        # Select the current word and add a space after it:\n",
    "        thisword = thislist[j] + \" \"\n",
    "        # Append the word+space to the string us_corpus:\n",
    "        us_corpus += thisword\n",
    "\n",
    "# Use ngrams() to look for two-word phrases in the string us_corpus, tokenizing the string in the method call:\n",
    "us_corpus_bigrams = ngrams(word_tokenize(us_corpus), 2)\n",
    "# Use list comprehension to generate a list from the iterator us_corpus_bigrams:\n",
    "us_corpus_bigrams = [' '.join(bigrams) for bigrams in us_corpus_bigrams]\n",
    "# Instantiate a Counter object using the list of two-word phrases us_corpus_bigrams:\n",
    "us_corpus_bigrams_frequency = Counter(us_corpus_bigrams)\n",
    "\n",
    "# Print out the 25 most common two-word phrases using the .most_common() method:\n",
    "print(\"United States Corpus Bigrams:\")\n",
    "print(us_corpus_bigrams_frequency.most_common(25))\n",
    "\n",
    "# Use ngrams() to look for three-word phrases in the string us_corpus, tokenizing the string in the method call:\n",
    "us_corpus_trigrams = ngrams(word_tokenize(us_corpus), 3)\n",
    "# Use list comprehension to generate a list from the iterator us_corpus_trigrams:\n",
    "us_corpus_trigrams = [' '.join(trigrams) for trigrams in us_corpus_trigrams]\n",
    "# Instantiate a Counter object using the list of three-word phrases us_corpus_trigrams:\n",
    "us_corpus_trigrams_frequency = Counter(us_corpus_trigrams)\n",
    "\n",
    "# Print out the 25 most common three-word phrases using the .most_common() method:\n",
    "print(\"United States Corpus Trigrams:\")\n",
    "print(us_corpus_trigrams_frequency.most_common(25))\n",
    "\n",
    "# Use ngrams() to look for four-word phrases in the string us_corpus, tokenizing the string in the method call:\n",
    "us_corpus_quadgrams = ngrams(word_tokenize(us_corpus), 4)\n",
    "# Use list comprehension to generate a list from the iterator us_corpus_quadgrams:\n",
    "us_corpus_quadgrams = [' '.join(grams) for grams in us_corpus_quadgrams]\n",
    "# Instantiate a Counter object using the list of four-word phrases us_corpus_quadgrams:\n",
    "us_corpus_quadgrams_frequency = Counter(us_corpus_quadgrams)\n",
    "\n",
    "# Print out the 15 most common three-word phrases using the .most_common() method:\n",
    "print(\"United States Corpus Four=grams:\")\n",
    "print(us_corpus_quadgrams_frequency.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common two-, three-, and four-word groups of words found in text messages sent by people from the United States is intriguing. I get the impression that the corpus is dominated by text messages from one or two people; I infer this because of very specific names that appear in the most common word groups (Ricky, Sue, Ing Cheong, Yeng Chin, Stewart) and what appears to be a series of text messages contacting someone about renting an apartment or condominium (\"hi interest villa verde,\" \"interest villa verde unit,\" \"build gsf area thanks\"). From the common groups of words I found, I infer that the dominant topic of these text messages is not to set up meetings or to ask someone if they want to do something; rather, they seem to be functioning in the same manner as chat services like WhatsApp, SnapChat, or (back in the old days of the Internet) AOL Instant Messages. There are some word groups that signal meetings and/or get togethers being discussed (\"next week,\" \"come home,\" \"go back\"), but many more indicative of short chat messages (\"thanks hi,\" \"ok thanks,\" \"lol u\", \"take good care,\" \"wish great day,\" \"happy new year\"). As I previously mentioned, there is some evidence that one or more people is/are inquiring about renting an apartment or condo from a Villa Verde building; some of the word groups like \"thanks hi ricky,\" \"please let know,\" \"email address email,\" and \"thanks hi stewart\" may be associated with these rental inquiries. I find the phrase \"gentleman treat dignity respect\" interesting, especially given that it appeared so many times in the corpus, though I cannot hazard a guess as to what sort of conversation it might be indicating. In short, text messages in the corpus of messages sent by people from the United States may be dominated by one or two people; clearly, one or more message senders is trying to rent an apartment or condominium, and the majority of the other most common two-, three-, and four-word phrases indicate that the message senders are merely chatting with loved ones or friends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section10\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Using Message Text to Predict Country of Origin </h3> <br>\n",
    "The last question I wanted to answer as part of my project was, \"Is it possible to use text message data to predict what country a sender is from?\" I thought that perhaps patterns of word usage, vocabulary, and prevalence of text messaging abbreviations could help one determine what country the sender of a particular text message was from. I also thought it likely that a supervised machine learning model could \"learn\" to tell the difference between, say, text messages sent from Singaporeans and text messages sent from Americans fairly readily, but it might have a harder time distinguishing between Singaporean and Malaysian text messages because they share a common language and the two countries are located so close to each other. My plan will be to use the raw text messages themselves—i.e., the un-processed and un-normalized original text messages—to train a Mutinomial Naive Bayes supervised machine learning model. A Multinomial Naive Bayes model is well-suited for categorization tasks, and it readily accepts text data once that data has been converted to count frequencies; this is easily done using the scikit-learn class CountVectorizer(). <br> \n",
    "<br> However, I must take some care in choosing the data I \"feed\" my Multinomial Naive Bayes model. Many countries have only a handful of text messages associated with them, whereas the corpus of text messages associated with Singapore has over 30,000 messages; due to the overwhelming number of messages from Singapore, the model will be able to readily \"learn\" the patterns of text messages associated with Singapore but will struggle when the country's corpus has only a handful of text messages. To try and ensure that there are enough messages in a country's corpus for the supervised machine learning model to \"learn\" how to identify the country, I will limit myself to considering only countries whose text message corpus has at least 100 messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore              31814\n",
      "India                   7085\n",
      "United States           5679\n",
      "Sri Lanka               1047\n",
      "Malaysia                 767\n",
      "Pakistan                 751\n",
      "unknown                  602\n",
      "Canada                   198\n",
      "Bangladesh               126\n",
      "China                    107\n",
      "Philippines               67\n",
      "Indonesia                 48\n",
      "United Kingdom            40\n",
      "Nepal                     39\n",
      "Hungary                   28\n",
      "Serbia                    22\n",
      "Kenya                     20\n",
      "Ghana                     18\n",
      "Italy                     10\n",
      "Macedonia                 10\n",
      "Lebanon                   10\n",
      "Slovenia                  10\n",
      "Trinidad and Tobago       10\n",
      "New Zealand               10\n",
      "Turkey                    10\n",
      "Nigeria                   10\n",
      "Australia                  9\n",
      "Romania                    9\n",
      "Morocco                    9\n",
      "Jamaica                    8\n",
      "Barbados                   8\n",
      "France                     5\n",
      "Spain                      5\n",
      "Name: country, dtype: int64\n",
      "\n",
      "Singapore        31814\n",
      "India             7085\n",
      "United States     5679\n",
      "Sri Lanka         1047\n",
      "Malaysia           767\n",
      "Pakistan           751\n",
      "unknown            602\n",
      "Canada             198\n",
      "Bangladesh         126\n",
      "China              107\n",
      "Name: country, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Which countries of origin are associated with at least 100 text messages?\n",
    "\n",
    "# Print out the number of texts from each country:\n",
    "print(sms_text_data.country.value_counts())\n",
    "\n",
    "# Select records only from those countries which are associated with 100 or more texts:\n",
    "sms_text_data_over100 = sms_text_data[sms_text_data.country.isin(['Singapore','India','United States','Sri Lanka',\\\n",
    "                                                                  'Malaysia','Pakistan','unknown','Canada',\\\n",
    "                                                                  'Bangladesh','China'])]\n",
    "\n",
    "# Print out the number of texts from each country in the new DataFrame, just to make sure that nothing blatantly obvious\n",
    "# has gone wrong:\n",
    "print('')\n",
    "print(sms_text_data_over100.country.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use all text from every message in sms_text_data_over100, including punctuation marks and other symbols, when training my Multinomial Naive Bayes model. This is being done primarily to preserve things like the usage of \"2\" instead of \"to,\" emojis, use of symbols, and particular punctuation patterns which could be important in trying to determine a message's country of origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column of strings Message will need to be converted to a list of strings; this is done below:\n",
    "master_text_list = sms_text_data_over100.Message.to_list()\n",
    "# Similarly, convert the column country to a list:\n",
    "master_country_list = sms_text_data_over100.country.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multinomial Naive Bayes model I will build is a supervised machine learning model and thus needs data to be trained on. I will therefore split the text message data into training and test subsets. 85% of the data will be partitioned into the training subset, and 15% will be partitioned into the test subset and \"set aside\" until the model has been trained. An initial random state is provided to ensure the same splitting between training and test data subsets each time this code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation subsets. I am choosing to place 85% of the Message data into the training\n",
    "# subset, and 15% into the test subset. An initial random state is provided to ensure the same spliltting between\n",
    "# training and validation subsets each time the code is run:\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(master_text_list, master_country_list,\\\n",
    "                                                                 train_size = 0.85, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can construct and train the model (after first converting the text data into count frequencies):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer() to make a dictionary and convert words in the training and validation subsets to count \n",
    "# frequencies of words in that dictionary:\n",
    "# Instantiate an object of the CountVectorizer() class:\n",
    "countvector = CountVectorizer()\n",
    "# Fit the CountVectorizer object to the vocabulary in the training data:\n",
    "countvector.fit(train_data)\n",
    "# Transform the words in the training data subset to counts:\n",
    "train_counts = countvector.transform(train_data)\n",
    "# Transform the words in the validation data subset to counts:\n",
    "test_counts = countvector.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the arrays from CountVectorizer, as well as the known country \"labels\" to build a Multinomial Naïve Bayes \n",
    "# classifier.\n",
    "# Instantiate an object of the MultinomialNB class:\n",
    "mnb_classifier = MultinomialNB()\n",
    "# Train the MultinomialNB object on the training data subset:\n",
    "mnb_classifier.fit(train_counts, train_labels)\n",
    "# Use the trained model to make predictions for the validation subset:\n",
    "mnb_predictions = mnb_classifier.predict(test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did my Multinomial Naive Bayes model perform? I will analyze not only accuracy, but also precision, recall, and f1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB classifier accuracy: 0.77\n",
      "MNB Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Singapore       0.78      0.98      0.87      4807\n",
      "        India       0.78      0.41      0.54      1083\n",
      "United States       0.68      0.37      0.48       839\n",
      "    Sri Lanka       0.83      0.35      0.50       164\n",
      "     Malaysia       0.00      0.00      0.00       100\n",
      "     Pakistan       0.80      0.45      0.58        88\n",
      "      unknown       0.85      0.14      0.24        79\n",
      "       Canada       1.00      0.03      0.06        31\n",
      "   Bangladesh       0.00      0.00      0.00        15\n",
      "        China       0.00      0.00      0.00        21\n",
      "\n",
      "     accuracy                           0.77      7227\n",
      "    macro avg       0.57      0.27      0.33      7227\n",
      " weighted avg       0.75      0.77      0.73      7227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a list of the names of the countries which were included in the DataFrame sms_text_data_over100:\n",
    "countries = ['Singapore','India','United States','Sri Lanka','Malaysia','Pakistan','unknown','Canada',\\\n",
    "             'Bangladesh','China']\n",
    "\n",
    "# Determine the model's accuracy and print it to screen:\n",
    "print('MNB classifier accuracy: ' + str(round(accuracy_score(test_labels,mnb_predictions),2)))\n",
    "# Use the method classification_report() to calculate the precision, recall, and f1 scores of each country\n",
    "# in the validation set. As a bonus, the model accuracy will also be included; this should match the number\n",
    "# calculated immediately above:\n",
    "print('MNB Classification Report:')\n",
    "print(classification_report(test_labels, mnb_predictions, labels=countries, target_names=countries, \\\n",
    "                            zero_division = 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy of my Multinomial Naive Bayes model is 77%, which, while much better than guessing a country at random (a 10% chance of being correct) is in absolute terms mediocre. Indeed, a little further digging reveals several flaws with the model. First of all, it was unable to identify any text messages from people from Malaysia; it misclassified every single one of those 100 messages as having been sent by people from some other country (most likely Singapore, given Malaysia’s proximity to Singapore). This isn’t entirely unsurprising; however, the model could also not identify any messages sent by people from Bangladesh and China. There were fewer messages from people from these countries in the corpus, so perhaps it should not be surprising that the model had a hard time identifying them as well. I note that, while the model did not misclassify any text messages as having been sent by someone from Canada—that is to say, the false positive rate for Canadian messages was 0—it misclassified an awful lot of text messages truly sent by people from Canada; that is to say, the recall for Canadian messages was atrocious. The recall score for messages sent by people from \"unknown\" was only marginally better. In fact, only Singapore had a recall score above 50%, meaning that the model did not misclassify many messages from people from Singapore as being associated with some other country. (The model did, however, have a higher \"false positive\" rate for messages classified as having been sent from people from Singapore, as evidenced by the precision score of 78%.) The macro average precision and recall scores for the model are dismal at 57% and 27%, respectively; however, these are unweighted averages that do not take label imbalance into account. The weighted average precision and recall do take the label imbalance into account, and they paint a slightly rosier picture with a weighted average precision of 75% and a weighted average recall of 77%. (The latter is of course being dominated by the very high recall score for messages from people from Singapore; this country dominates the corpus.) <br>\n",
    "In short, the Multinomial Naive Bayes model I built is, perhaps unsurprisingly, good at finding messages from people from Singapore and labeling them as such; it has a much harder time differentiating between other countries. I say that this result probably isn't surprising because text messages from people from Singapore dominate the corpus of text messages that I trained the model on—given so many messages from people from Singapore in the training set, the model should be good at differentiating those messages from other messages. In fact, there are three countries whose associated text messages were all misclassified; this is a decided flaw of my model. Thus in absolute terms, the model I built is mediocre at best and is really only well suited to finding messages sent by people from Singapore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section11\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> Conclusions </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the National University of Singapore SMS Corpus data set to practice asking questions about text-based data sets and using natural language processing to answer these questions. A variety of techniques were used to gain insight into the topics of text messages in the data set, including word count/term frequency (TF), term frequency-inverse term frequency (TF-IDF), word embeddings, and n-gram analysis. I developed the following questions and used the above methods to attempt to answer them:<br>\n",
    "<ol>\n",
    "    <li> Text message senders from which countries send the longest messages? </li>\n",
    "    People from Canada sent text messages which, on average, contained the most characters. People from Sri Lanka, Macedonia, Ghana, and Singapore round out the top five longest average text messages.\n",
    "    <li> What are some of the most common topics of text messages sent by people from Singapore, India, and the United States? How are they similar, and how are they different?</li>\n",
    "    People from Singapore seem to primarily use text messages primarily to set up meetings and/or get-togethers, and to ask people if they would like to do something. The other two most common text message topics were reactions to events/memories/shared information, and to send well wishes to loved ones. Some people from India seem to have been using a text message sending service to send their messages, which seem to commonly be about either setting up meetings/get togethers or setting up a time to call the message receiver. People from the United States also use text messages to set up meetings/get-togethers, but seem to use them more for chatting and to attend to personal matters such as finding an apartment to rent. \n",
    "    <li> How (if at all) did the common topics of text messages sent by people from Singapore change between 2003 and 2011?</li>\n",
    "    The messages sent by people from Singapore in 2003 seem to have been primarily about setting up meetings and/or get-togethers, or to tell a receiver that they will be late for a meeting. Messages from people from Singapore still dealt with meetings/get-togethers in 2011, but messages that seemed to be reacting to events, memories, links, or other shared information became more common.\n",
    "    <li> Is it possible to use text message data to predict what country a sender is from?</li>\n",
    "    The Multinomial Naive Bayes model I built was fairly successful at identifying messages sent by people from Singapore as having been associated with Singapore, but had a much harder time correctly classifying text messages associated with senders from other countries. Such a model is of very limited use in trying to predict what country a sender is from, so I consider it marginally successful at best.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Section12\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\"> What I Would Do Differently </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a former physics instructor, I am well aware that the best way to learn how to do something is to do it. From our first attempts at solving physics problems we gain insights into how to be more organized and efficient when solving a particular class of problem; these insights can be applied to future problems, hopefully allowing more thoughtful solutions and pattern recognition synthesis. Data science projects are, of course, no different. As a result of completing my first independent natural language processing project, I gained several insights which will serve me well in future projects. <br>\n",
    "<br>\n",
    "After I completed the analysis, I realized that there were much faster and efficient ways to conduct it. Were I to attempt this project (or one like it) again, I would first collect data from the countries I want to look at, then put those messages into lists; this would save a lot of unnecessary processing of texts from the remaining countries, not to mention allow more efficient tokenizing, stopwords removal, and lemmatization. I noticed that I did a great deal of converting tokenized lists of words into lists of strings, and having taken a list-based approach rather than a DataFrame column approach would have made that process much more efficient. Those lists of strings or lists of tokenized text messages could be added to a DataFrame later if such became necessary. <br>\n",
    "<br>\n",
    "Second, I would focus more on one strategy of text message topic analysis rather than throwing the proverbial kitchen sink at the problem. In retrospect, n-gram identification was probably the most useful analysis I did, with TF/TF-IDF second-most useful. <br>\n",
    "<br>\n",
    "Lastly, were I to do this project (or one like it) again, I would look into different ways to make predictions using the text message data than just Multinomial Naive Bayes, which is my go-to categorization method to use when text-based data is involved. Perhaps there is something that is a bit less sensitive to being dominated by one particular category of data, or which is a bit more sensitive to the nuanced differences in word use and word grouping between text messages from people from different countries. <br>\n",
    "<br>\n",
    "I will mention in passing that it would have been interesting to work with the messages sent by people from countries other than Singapore, India, and the United States. Of particular interest to me is how similar messages from people from Singapore are to messages from people from Malaysia, given how close in physical proximity the two countries are; it might also have been interesting to compare and contrast text messages from people from countries which have very different climates and geographies, such as Singapore and Spain or Trinidad and Tobago and Nepal. The drawback there is, of course, that many of these countries are associated with only a handful of messages, and so the likelihood that I would be comparing the text messages of one individual from, say, Trinidad and Tobago to one individual from, say, Nepal, is not insignificant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
